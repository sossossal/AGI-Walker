"""
ONNXæ¨ç†å¼•æ“
é«˜æ€§èƒ½æ¨ç†ï¼Œæ”¯æŒCPUå’ŒGPUï¼Œæœ€å¤§å…¼å®¹æ€§
"""

import time
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass
from collections import deque


@dataclass
class ONNXConfig:
    """ONNXæ¨ç†é…ç½®"""
    use_gpu: bool = False  # é»˜è®¤ä½¿ç”¨CPUä»¥ä¿è¯æœ€å¤§å…¼å®¹æ€§
    num_threads: int = 4
    enable_profiling: bool = False
    graph_optimization_level: str = "all"  # "basic", "extended", "all"


class ONNXInferenceEngine:
    """
    é«˜æ€§èƒ½ONNXæ¨ç†å¼•æ“
    
    ç‰¹ç‚¹ï¼š
    - è‡ªåŠ¨æ£€æµ‹å¯ç”¨Provider
    - æœ€å¤§å…¼å®¹æ€§ï¼ˆä¼˜å…ˆä½¿ç”¨CPUï¼‰
    - å»¶è¿Ÿç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        config: Optional[ONNXConfig] = None
    ):
        self.config = config or ONNXConfig()
        self.model_path = model_path
        self.session = None
        self.input_names: List[str] = []
        self.output_names: List[str] = []
        self.input_shapes: Dict[str, Tuple] = {}
        
        # å»¶è¿Ÿåˆå§‹åŒ–ONNX Runtime
        self._ort = None
        self._init_onnx_runtime()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.latency_history: deque = deque(maxlen=1000)
        self.total_inferences = 0
        self.total_time = 0.0
        
        # åŠ è½½æ¨¡å‹
        if model_path and Path(model_path).exists():
            self.load_model(model_path)
    
    def _init_onnx_runtime(self):
        """åˆå§‹åŒ–ONNX Runtime"""
        try:
            import onnxruntime as ort
            self._ort = ort
            
            # æ£€æµ‹å¯ç”¨Providers
            available_providers = ort.get_available_providers()
            print(f"ğŸ“¦ ONNX Runtimeç‰ˆæœ¬: {ort.__version__}")
            print(f"å¯ç”¨Providers: {available_providers}")
            
        except ImportError:
            print("âš ï¸ ONNX Runtimeæœªå®‰è£…")
            print("è¯·è¿è¡Œ: pip install onnxruntime")
            self._ort = None
    
    def load_model(self, model_path: str):
        """åŠ è½½ONNXæ¨¡å‹"""
        if self._ort is None:
            print("âŒ ONNX Runtimeä¸å¯ç”¨")
            return False
        
        path = Path(model_path)
        if not path.exists():
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        # é…ç½®Sessioné€‰é¡¹
        sess_options = self._ort.SessionOptions()
        sess_options.intra_op_num_threads = self.config.num_threads
        
        # å›¾ä¼˜åŒ–çº§åˆ«
        opt_levels = {
            "basic": self._ort.GraphOptimizationLevel.ORT_ENABLE_BASIC,
            "extended": self._ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED,
            "all": self._ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        }
        sess_options.graph_optimization_level = opt_levels.get(
            self.config.graph_optimization_level,
            self._ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        )
        
        if self.config.enable_profiling:
            sess_options.enable_profiling = True
        
        # é€‰æ‹©Providerï¼ˆæœ€å¤§å…¼å®¹æ€§ï¼‰
        providers = self._get_providers()
        
        try:
            self.session = self._ort.InferenceSession(
                str(path),
                sess_options,
                providers=providers
            )
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            self.input_names = [inp.name for inp in self.session.get_inputs()]
            self.output_names = [out.name for out in self.session.get_outputs()]
            
            for inp in self.session.get_inputs():
                self.input_shapes[inp.name] = inp.shape
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"è¾“å…¥: {self.input_names}")
            print(f"è¾“å‡º: {self.output_names}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _get_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„Providersï¼ˆæœ€å¤§å…¼å®¹æ€§ï¼‰"""
        if self._ort is None:
            return []
        
        available = self._ort.get_available_providers()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆCPUä¼˜å…ˆä»¥ä¿è¯å…¼å®¹æ€§ï¼‰
        priority = [
            'CPUExecutionProvider',
            'CUDAExecutionProvider',
            'TensorrtExecutionProvider',
            'DmlExecutionProvider',  # Windows DirectML
            'CoreMLExecutionProvider',  # macOS
        ]
        
        if self.config.use_gpu:
            # GPUä¼˜å…ˆ
            priority = [
                'CUDAExecutionProvider',
                'TensorrtExecutionProvider',
                'DmlExecutionProvider',
                'CPUExecutionProvider',
            ]
        
        providers = []
        for p in priority:
            if p in available:
                providers.append(p)
        
        if not providers:
            providers = ['CPUExecutionProvider']
        
        print(f"ä½¿ç”¨Providers: {providers}")
        return providers
    
    def predict(
        self,
        inputs: Union[np.ndarray, Dict[str, np.ndarray]]
    ) -> Union[np.ndarray, Dict[str, np.ndarray]]:
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            inputs: è¾“å…¥æ•°æ®ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ•°ç»„æˆ–å­—å…¸
        
        Returns:
            æ¨ç†ç»“æœ
        """
        if self.session is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        
        # å‡†å¤‡è¾“å…¥
        if isinstance(inputs, np.ndarray):
            # å•è¾“å…¥
            feed_dict = {self.input_names[0]: inputs.astype(np.float32)}
        else:
            # å¤šè¾“å…¥
            feed_dict = {
                name: data.astype(np.float32) 
                for name, data in inputs.items()
            }
        
        # æ‰§è¡Œæ¨ç†
        outputs = self.session.run(self.output_names, feed_dict)
        
        # ç»Ÿè®¡
        latency = time.time() - start_time
        self.latency_history.append(latency)
        self.total_inferences += 1
        self.total_time += latency
        
        # è¿”å›ç»“æœ
        if len(outputs) == 1:
            return outputs[0]
        else:
            return {name: out for name, out in zip(self.output_names, outputs)}
    
    def predict_batch(
        self,
        inputs: np.ndarray,
        batch_size: int = 32
    ) -> np.ndarray:
        """æ‰¹é‡æ¨ç†"""
        num_samples = inputs.shape[0]
        results = []
        
        for i in range(0, num_samples, batch_size):
            batch = inputs[i:i+batch_size]
            result = self.predict(batch)
            results.append(result)
        
        return np.concatenate(results, axis=0)
    
    def warmup(self, num_runs: int = 10):
        """æ¨¡å‹é¢„çƒ­"""
        if self.session is None:
            return
        
        print(f"æ¨¡å‹é¢„çƒ­ ({num_runs}æ¬¡)...")
        
        # åˆ›å»ºdummyè¾“å…¥
        dummy_inputs = {}
        for name, shape in self.input_shapes.items():
            # æ›¿æ¢åŠ¨æ€ç»´åº¦ä¸º1
            static_shape = [s if isinstance(s, int) and s > 0 else 1 for s in shape]
            dummy_inputs[name] = np.random.randn(*static_shape).astype(np.float32)
        
        for _ in range(num_runs):
            self.predict(dummy_inputs)
        
        # æ¸…é™¤é¢„çƒ­ç»Ÿè®¡
        self.latency_history.clear()
        self.total_inferences = 0
        self.total_time = 0.0
        
        print("âœ… é¢„çƒ­å®Œæˆ")
    
    def get_latency_stats(self) -> dict:
        """è·å–å»¶è¿Ÿç»Ÿè®¡"""
        if not self.latency_history:
            return {
                "avg_ms": 0,
                "min_ms": 0,
                "max_ms": 0,
                "p50_ms": 0,
                "p95_ms": 0,
                "p99_ms": 0,
                "total_inferences": 0
            }
        
        latencies = list(self.latency_history)
        latencies.sort()
        
        return {
            "avg_ms": sum(latencies) / len(latencies) * 1000,
            "min_ms": min(latencies) * 1000,
            "max_ms": max(latencies) * 1000,
            "p50_ms": latencies[len(latencies) // 2] * 1000,
            "p95_ms": latencies[int(len(latencies) * 0.95)] * 1000,
            "p99_ms": latencies[int(len(latencies) * 0.99)] * 1000,
            "total_inferences": self.total_inferences,
            "throughput_fps": self.total_inferences / self.total_time if self.total_time > 0 else 0
        }
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.session is None:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "model_path": self.model_path,
            "inputs": {
                name: {"shape": self.input_shapes.get(name)}
                for name in self.input_names
            },
            "outputs": self.output_names,
            "providers": self.session.get_providers()
        }


class DummyONNXEngine:
    """è™šæ‹ŸONNXå¼•æ“ï¼ˆå½“ONNX Runtimeä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
    
    def __init__(self, *args, **kwargs):
        print("âš ï¸ ä½¿ç”¨è™šæ‹ŸONNXå¼•æ“")
        self.total_inferences = 0
    
    def load_model(self, model_path: str) -> bool:
        return False
    
    def predict(self, inputs: np.ndarray) -> np.ndarray:
        self.total_inferences += 1
        # è¿”å›é›¶è¾“å‡º
        return np.zeros((inputs.shape[0], 1), dtype=np.float32)
    
    def get_latency_stats(self) -> dict:
        return {"dummy": True, "total_inferences": self.total_inferences}
    
    def get_model_info(self) -> dict:
        return {"loaded": False, "dummy": True}


def create_onnx_engine(
    model_path: Optional[str] = None,
    use_gpu: bool = False
) -> ONNXInferenceEngine:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºONNXæ¨ç†å¼•æ“
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
    
    Returns:
        ONNXæ¨ç†å¼•æ“å®ä¾‹
    """
    try:
        import onnxruntime
        config = ONNXConfig(use_gpu=use_gpu)
        return ONNXInferenceEngine(model_path, config)
    except ImportError:
        return DummyONNXEngine()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import json
    
    print("ONNXæ¨ç†å¼•æ“æµ‹è¯•\n")
    
    # åˆ›å»ºå¼•æ“
    engine = create_onnx_engine(use_gpu=False)
    
    print("\n=== æ¨¡å‹ä¿¡æ¯ ===")
    print(json.dumps(engine.get_model_info(), indent=2))
    
    # å¦‚æœæœ‰ç°æœ‰ONNXæ¨¡å‹ï¼Œæµ‹è¯•æ¨ç†
    reflex_model = Path("d:/æ–°å»ºæ–‡ä»¶å¤¹/AGI-Walker/hive-reflex/reflex_net.onnx")
    
    if reflex_model.exists():
        print(f"\n=== æµ‹è¯•ReflexNetæ¨¡å‹ ===")
        
        if engine.load_model(str(reflex_model)):
            # é¢„çƒ­
            engine.warmup(10)
            
            # æµ‹è¯•æ¨ç†
            dummy_input = np.random.randn(1, 5, 12).astype(np.float32)
            h0 = np.zeros((1, 1, 16), dtype=np.float32)
            c0 = np.zeros((1, 1, 16), dtype=np.float32)
            
            for _ in range(100):
                output = engine.predict({
                    "input": dummy_input,
                    "h_in": h0,
                    "c_in": c0
                })
            
            print("\nå»¶è¿Ÿç»Ÿè®¡:")
            print(json.dumps(engine.get_latency_stats(), indent=2))
    else:
        print(f"\nâš ï¸ ReflexNet ONNXæ¨¡å‹ä¸å­˜åœ¨")
        print(f"è¯·è¿è¡Œ: cd hive-reflex && python reflex_net.py")
        
        # æµ‹è¯•è™šæ‹Ÿæ¨ç†
        if isinstance(engine, DummyONNXEngine):
            print("\nä½¿ç”¨è™šæ‹Ÿå¼•æ“æµ‹è¯•...")
            dummy_input = np.random.randn(1, 12).astype(np.float32)
            output = engine.predict(dummy_input)
            print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
