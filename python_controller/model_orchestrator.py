"""
æ¨¡å‹ç¼–æ’å™¨ï¼ˆModel Orchestratorï¼‰
åè°ƒä¸‰å±‚AIæ¨¡å‹ï¼šå°æ¨¡å‹ï¼ˆ3Bï¼‰ã€ä¸­æ¨¡å‹ï¼ˆ7Bï¼‰ã€å¤§æ¨¡å‹ï¼ˆ70Bï¼‰
å®ç°åŠ¨æ€æ¨¡å‹é€‰æ‹©å’Œæ™ºèƒ½è·¯ç”±
"""

import time
from typing import Dict, Optional, Literal, Callable
from dataclasses import dataclass
from enum import Enum
from ai_model import BaseAIModel, OllamaModel, create_ai_model
from medium_model import MediumModel


class ModelTier(Enum):
    """æ¨¡å‹å±‚çº§"""
    SMALL = "small"      # 3B - å®æ—¶æ§åˆ¶
    MEDIUM = "medium"    # 7B - åŠå®æ—¶ä»»åŠ¡
    LARGE = "large"      # 70B - ç¦»çº¿ä¼˜åŒ–


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    small_model: str = "phi3:mini"      # 3Bå°æ¨¡å‹
    medium_model: str = "mistral:7b"    # 7Bä¸­æ¨¡å‹
    large_model: str = "llama2:70b"     # 70Bå¤§æ¨¡å‹
    
    # å»¶è¿Ÿé˜ˆå€¼ï¼ˆmsï¼‰
    small_latency_threshold: float = 50.0
    medium_latency_threshold: float = 500.0
    
    # è°ƒç”¨é¢‘ç‡é™åˆ¶
    medium_min_interval: float = 0.5    # ä¸­æ¨¡å‹æœ€å°è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
    large_min_interval: float = 60.0    # å¤§æ¨¡å‹æœ€å°è°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰


class ModelOrchestrator:
    """
    ä¸‰å±‚æ¨¡å‹ç¼–æ’å™¨
    
    è´Ÿè´£ï¼š
    - æ¨¡å‹é€‰æ‹©å’Œè·¯ç”±
    - å»¶è¿Ÿç›‘æ§å’Œé™çº§
    - æ—¥å¿—æ”¶é›†å’Œä¸ŠæŠ¥
    - ç­–ç•¥åŒæ­¥
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        
        # æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._small_model: Optional[BaseAIModel] = None
        self._medium_model: Optional[MediumModel] = None
        self._large_model: Optional[BaseAIModel] = None
        
        # çŠ¶æ€è·Ÿè¸ª
        self.last_medium_call = 0.0
        self.last_large_call = 0.0
        self.current_tier = ModelTier.SMALL
        
        # ç»Ÿè®¡
        self.tier_usage = {
            ModelTier.SMALL: 0,
            ModelTier.MEDIUM: 0,
            ModelTier.LARGE: 0
        }
        self.total_requests = 0
        self.fallback_count = 0
        
        # å›è°ƒå‡½æ•°
        self.on_escalate: Optional[Callable] = None
        self.on_tier_change: Optional[Callable] = None
    
    # =================== æ¨¡å‹è®¿é—®å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ ===================
    
    @property
    def small_model(self) -> BaseAIModel:
        """è·å–å°æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if self._small_model is None:
            print("æ­£åœ¨åŠ è½½å°æ¨¡å‹...")
            self._small_model = create_ai_model(
                engine="ollama",
                model_name=self.config.small_model
            )
        return self._small_model
    
    @property
    def medium_model(self) -> MediumModel:
        """è·å–ä¸­æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼‰"""
        if self._medium_model is None:
            print("æ­£åœ¨åŠ è½½ä¸­æ¨¡å‹...")
            self._medium_model = MediumModel(
                model_name=self.config.medium_model
            )
        return self._medium_model
    
    @property
    def large_model(self) -> Optional[BaseAIModel]:
        """è·å–å¤§æ¨¡å‹ï¼ˆæŒ‰éœ€åŠ è½½ï¼Œå¯èƒ½ä¸ºNoneï¼‰"""
        if self._large_model is None:
            try:
                print("æ­£åœ¨åŠ è½½å¤§æ¨¡å‹...")
                self._large_model = create_ai_model(
                    engine="ollama",
                    model_name=self.config.large_model
                )
            except Exception as e:
                print(f"âš ï¸ å¤§æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("å¤§æ¨¡å‹åŠŸèƒ½å°†è¢«ç¦ç”¨")
                return None
        return self._large_model
    
    # =================== æ ¸å¿ƒå¤„ç†æ¥å£ ===================
    
    def process(self, sensor_data: dict, context: str = "realtime") -> dict:
        """
        æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤„ç†è¯·æ±‚
        
        Args:
            sensor_data: ä¼ æ„Ÿå™¨æ•°æ®
            context: ä¸Šä¸‹æ–‡ç±»å‹ ("realtime", "adjustment", "optimization")
        
        Returns:
            å¤„ç†ç»“æœ
        """
        self.total_requests += 1
        
        if context == "realtime":
            return self._process_realtime(sensor_data)
        elif context == "adjustment":
            return self._process_adjustment(sensor_data)
        elif context == "optimization":
            return self._process_optimization(sensor_data)
        else:
            print(f"âš ï¸ æœªçŸ¥ä¸Šä¸‹æ–‡: {context}ï¼Œä½¿ç”¨å®æ—¶å¤„ç†")
            return self._process_realtime(sensor_data)
    
    def _process_realtime(self, sensor_data: dict) -> dict:
        """å®æ—¶æ§åˆ¶å¤„ç†ï¼ˆå°æ¨¡å‹ï¼‰"""
        start_time = time.time()
        
        try:
            result = self.small_model.predict(sensor_data)
            latency = (time.time() - start_time) * 1000
            
            # è®°å½•å»¶è¿Ÿç”¨äºç›‘æ§
            result['_latency_ms'] = latency
            result['_tier'] = ModelTier.SMALL.value
            
            self.tier_usage[ModelTier.SMALL] += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
            if self._should_trigger_adjustment(sensor_data, latency):
                self._schedule_adjustment(sensor_data)
            
            return result
            
        except Exception as e:
            print(f"âŒ å°æ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            self.fallback_count += 1
            return self._get_fallback_action()
    
    def _process_adjustment(self, sensor_data: dict) -> dict:
        """ç¯å¢ƒæ„ŸçŸ¥è°ƒæ•´ï¼ˆä¸­æ¨¡å‹ï¼‰"""
        current_time = time.time()
        
        # æ£€æŸ¥è°ƒç”¨é—´éš”
        if current_time - self.last_medium_call < self.config.medium_min_interval:
            return {"skip": True, "reason": "è°ƒç”¨é—´éš”è¿‡çŸ­"}
        
        start_time = time.time()
        
        try:
            result = self.medium_model.adjust_environment(sensor_data)
            latency = (time.time() - start_time) * 1000
            
            result['_latency_ms'] = latency
            result['_tier'] = ModelTier.MEDIUM.value
            
            self.last_medium_call = current_time
            self.tier_usage[ModelTier.MEDIUM] += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸ŠæŠ¥å¤§æ¨¡å‹
            if result.get('escalate', False):
                self._schedule_optimization(sensor_data, result)
            
            return result
            
        except Exception as e:
            print(f"âŒ ä¸­æ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            return {"error": str(e), "skip": True}
    
    def _process_optimization(self, sensor_data: dict) -> dict:
        """ç¦»çº¿ç­–ç•¥ä¼˜åŒ–ï¼ˆå¤§æ¨¡å‹ï¼‰"""
        current_time = time.time()
        
        # æ£€æŸ¥è°ƒç”¨é—´éš”
        if current_time - self.last_large_call < self.config.large_min_interval:
            return {"skip": True, "reason": "è°ƒç”¨é—´éš”è¿‡çŸ­"}
        
        if self.large_model is None:
            return {"skip": True, "reason": "å¤§æ¨¡å‹æœªåŠ è½½"}
        
        start_time = time.time()
        
        try:
            # æ”¶é›†æ—¥å¿—
            logs = self.medium_model.get_logs_for_large_model()
            
            # æ„å»ºä¼˜åŒ–è¯·æ±‚
            optimization_data = {
                **sensor_data,
                "logs": logs,
                "environment_state": self.medium_model.environment_state
            }
            
            result = self.large_model.predict(
                optimization_data,
                strategy="æ·±åº¦ç­–ç•¥ä¼˜åŒ–æ¨¡å¼"
            )
            
            latency = (time.time() - start_time) * 1000
            
            result['_latency_ms'] = latency
            result['_tier'] = ModelTier.LARGE.value
            
            self.last_large_call = current_time
            self.tier_usage[ModelTier.LARGE] += 1
            
            # è§¦å‘å›è°ƒ
            if self.on_escalate:
                self.on_escalate(result)
            
            return result
            
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹å¤„ç†å¤±è´¥: {e}")
            return {"error": str(e), "skip": True}
    
    # =================== è¾…åŠ©æ–¹æ³• ===================
    
    def _should_trigger_adjustment(self, sensor_data: dict, latency: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘ç¯å¢ƒè°ƒæ•´"""
        # å»¶è¿Ÿè¶…æ ‡
        if latency > self.config.small_latency_threshold:
            return True
        
        # å§¿æ€ä¸ç¨³å®š
        orient = sensor_data['sensors']['imu']['orient']
        if abs(orient[0]) > 15 or abs(orient[1]) > 15:
            return True
        
        # é«˜åº¦å¼‚å¸¸
        height = sensor_data.get('torso_height', 1.0)
        if height < 0.5:
            return True
        
        return False
    
    def _schedule_adjustment(self, sensor_data: dict):
        """è°ƒåº¦ç¯å¢ƒè°ƒæ•´ï¼ˆå¼‚æ­¥ï¼‰"""
        # ç®€å•å®ç°ï¼šç›´æ¥è°ƒç”¨
        # å®Œæ•´å®ç°åº”ä½¿ç”¨çº¿ç¨‹æ± æˆ–å¼‚æ­¥é˜Ÿåˆ—
        self._process_adjustment(sensor_data)
    
    def _schedule_optimization(self, sensor_data: dict, adjustment_result: dict):
        """è°ƒåº¦ç¦»çº¿ä¼˜åŒ–ï¼ˆå¼‚æ­¥ï¼‰"""
        print("ğŸ“¤ è°ƒåº¦å¤§æ¨¡å‹ä¼˜åŒ–...")
        # å®Œæ•´å®ç°åº”ä½¿ç”¨åå°çº¿ç¨‹
        self._process_optimization(sensor_data)
    
    def _get_fallback_action(self) -> dict:
        """è·å–fallbackåŠ¨ä½œ"""
        return {
            "motors": {"hip_left": 0.0, "hip_right": 0.0},
            "confidence": 0.0,
            "_tier": "fallback"
        }
    
    # =================== æ—¥å¿—ç®¡ç† ===================
    
    def add_log(self, log_entry: dict):
        """æ·»åŠ æ—¥å¿—"""
        self.medium_model.add_log(log_entry)
    
    def filter_logs(self, logs: list) -> list:
        """è¿‡æ»¤æ—¥å¿—"""
        return self.medium_model.filter_logs(logs)
    
    # =================== ç»Ÿè®¡å’Œç›‘æ§ ===================
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_requests": self.total_requests,
            "tier_usage": {k.value: v for k, v in self.tier_usage.items()},
            "fallback_count": self.fallback_count,
            "current_tier": self.current_tier.value,
            "small_model_stats": self.small_model.get_stats() if self._small_model else None,
            "medium_model_stats": self.medium_model.get_stats() if self._medium_model else None,
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.tier_usage = {tier: 0 for tier in ModelTier}
        self.total_requests = 0
        self.fallback_count = 0


def create_orchestrator(
    small_model: str = "phi3:mini",
    medium_model: str = "mistral:7b",
    large_model: str = "llama2:70b"
) -> ModelOrchestrator:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ¨¡å‹ç¼–æ’å™¨
    
    Args:
        small_model: å°æ¨¡å‹åç§°
        medium_model: ä¸­æ¨¡å‹åç§°
        large_model: å¤§æ¨¡å‹åç§°
    
    Returns:
        ModelOrchestratorå®ä¾‹
    """
    config = ModelConfig(
        small_model=small_model,
        medium_model=medium_model,
        large_model=large_model
    )
    return ModelOrchestrator(config)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import json
    
    print("æ¨¡å‹ç¼–æ’å™¨æµ‹è¯•\n")
    
    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = create_orchestrator(
        small_model="phi3:mini",
        medium_model="mistral:7b"
    )
    
    # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
    dummy_sensor = {
        "sensors": {
            "imu": {"orient": [5.2, -3.1, 0.0]},
            "joints": {
                "hip_left": {"angle": 10.0, "velocity": 0.0},
                "hip_right": {"angle": -8.0, "velocity": 0.0}
            }
        },
        "torso_height": 1.45
    }
    
    # æµ‹è¯•å®æ—¶å¤„ç†
    print("1. æµ‹è¯•å®æ—¶å¤„ç†ï¼ˆå°æ¨¡å‹ï¼‰...")
    result = orchestrator.process(dummy_sensor, context="realtime")
    print(f"ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•ç¯å¢ƒè°ƒæ•´
    print("\n2. æµ‹è¯•ç¯å¢ƒè°ƒæ•´ï¼ˆä¸­æ¨¡å‹ï¼‰...")
    result = orchestrator.process(dummy_sensor, context="adjustment")
    print(f"ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
    
    # ç»Ÿè®¡
    print("\n3. ç»Ÿè®¡ä¿¡æ¯:")
    stats = orchestrator.get_stats()
    print(json.dumps(stats, indent=2, ensure_ascii=False, default=str))
