"""
RAGç‰©ç†çŸ¥è¯†åº“ï¼ˆRetrieval-Augmented Generationï¼‰
ç¦»çº¿éƒ¨ç½²ç‰ˆæœ¬ï¼Œç”¨äºå¢å¼ºå¤§æ¨¡å‹çš„ç‰©ç†çŸ¥è¯†
"""

import os
import json
import pickle
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path


@dataclass
class KnowledgeEntry:
    """çŸ¥è¯†æ¡ç›®"""
    id: str
    category: str
    title: str
    content: str
    keywords: List[str]
    embedding: Optional[List[float]] = None


class PhysicsKnowledgeBase:
    """
    ç‰©ç†çŸ¥è¯†åº“RAGå¢å¼ºæ¨¡å—
    
    æ”¯æŒï¼š
    - ç¦»çº¿éƒ¨ç½²ï¼ˆæ— éœ€ç½‘ç»œï¼‰
    - æœ¬åœ°åµŒå…¥å‘é‡è®¡ç®—
    - å…³é”®è¯åŒ¹é… + è¯­ä¹‰æ£€ç´¢
    """
    
    # é¢„ç½®ç‰©ç†çŸ¥è¯†åº“
    BUILTIN_KNOWLEDGE = [
        # é‡åŠ›ä¸æƒ¯æ€§
        {
            "id": "gravity_001",
            "category": "gravity",
            "title": "é‡åŠ›åŠ é€Ÿåº¦",
            "content": "åœ°çƒè¡¨é¢é‡åŠ›åŠ é€Ÿåº¦gçº¦ä¸º9.8m/sÂ²ã€‚æœºå™¨äººå—åˆ°å‘ä¸‹çš„é‡åŠ›F=mgï¼Œå…¶ä¸­mä¸ºè´¨é‡ã€‚ä¿æŒå¹³è¡¡éœ€è¦åœ°é¢æä¾›ç­‰å¤§åå‘çš„æ”¯æ’‘åŠ›ã€‚",
            "keywords": ["é‡åŠ›", "åŠ é€Ÿåº¦", "g", "9.8", "è´¨é‡", "æ”¯æ’‘åŠ›"]
        },
        {
            "id": "gravity_002",
            "category": "gravity",
            "title": "é‡å¿ƒæ¦‚å¿µ",
            "content": "é‡å¿ƒ(Center of Mass, CoM)æ˜¯ç‰©ä½“è´¨é‡åˆ†å¸ƒçš„å‡ ä½•ä¸­å¿ƒã€‚åŒè¶³æœºå™¨äººéœ€ä¿æŒé‡å¿ƒæŠ•å½±è½åœ¨æ”¯æ’‘å¤šè¾¹å½¢å†…æ‰èƒ½ç¨³å®šç«™ç«‹ã€‚é‡å¿ƒé«˜åº¦è¶Šä½ï¼Œç¨³å®šæ€§è¶Šå¥½ã€‚",
            "keywords": ["é‡å¿ƒ", "CoM", "è´¨é‡ä¸­å¿ƒ", "æ”¯æ’‘å¤šè¾¹å½¢", "ç¨³å®šæ€§"]
        },
        {
            "id": "gravity_003",
            "category": "gravity",
            "title": "å€¾è¦†åŠ›çŸ©",
            "content": "å½“æœºå™¨äººå€¾æ–œè§’åº¦Î¸æ—¶ï¼Œé‡åŠ›äº§ç”Ÿå€¾è¦†åŠ›çŸ©M=mgh*sin(Î¸)ï¼Œå…¶ä¸­hä¸ºé‡å¿ƒé«˜åº¦ã€‚éœ€è¦å…³èŠ‚åŠ›çŸ©äº§ç”Ÿæ¢å¤åŠ›çŸ©æ¥æŠµæ¶ˆã€‚",
            "keywords": ["å€¾è¦†", "åŠ›çŸ©", "å€¾æ–œ", "è§’åº¦", "æ¢å¤åŠ›çŸ©"]
        },
        
        # æ‘©æ“¦åŠ›
        {
            "id": "friction_001",
            "category": "friction",
            "title": "é™æ‘©æ“¦åŠ›",
            "content": "é™æ‘©æ“¦åŠ›fâ‰¤Î¼sNï¼Œå…¶ä¸­Î¼sä¸ºé™æ‘©æ“¦ç³»æ•°ï¼ŒNä¸ºæ­£å‹åŠ›ã€‚æœºå™¨äººè„šåº•ä¸åœ°é¢çš„æ‘©æ“¦æ˜¯é˜²æ­¢æ»‘åŠ¨çš„å…³é”®ã€‚å…¸å‹æ©¡èƒ¶-æ··å‡åœŸé™æ‘©æ“¦ç³»æ•°ä¸º0.6-0.8ã€‚",
            "keywords": ["é™æ‘©æ“¦", "æ‘©æ“¦ç³»æ•°", "æ­£å‹åŠ›", "æ»‘åŠ¨", "æ©¡èƒ¶"]
        },
        {
            "id": "friction_002",
            "category": "friction",
            "title": "æ»‘åŠ¨æ‘©æ“¦",
            "content": "æ»‘åŠ¨æ‘©æ“¦åŠ›f=Î¼kNï¼Œå…¶ä¸­Î¼kä¸ºåŠ¨æ‘©æ“¦ç³»æ•°ï¼Œé€šå¸¸å°äºé™æ‘©æ“¦ç³»æ•°ã€‚ä¸€æ—¦å¼€å§‹æ»‘åŠ¨ï¼Œæ‘©æ“¦åŠ›é™ä½ï¼Œå®¹æ˜“å¤±æ§ã€‚",
            "keywords": ["æ»‘åŠ¨æ‘©æ“¦", "åŠ¨æ‘©æ“¦", "å¤±æ§", "æ»‘åŠ¨"]
        },
        
        # å¹³è¡¡æ§åˆ¶
        {
            "id": "balance_001",
            "category": "balance",
            "title": "é›¶åŠ›çŸ©ç‚¹(ZMP)",
            "content": "é›¶åŠ›çŸ©ç‚¹(Zero Moment Point)æ˜¯åŠ¨æ€å¹³è¡¡çš„å…³é”®æŒ‡æ ‡ã€‚ZMPä½äºé‡åŠ›å’Œæƒ¯æ€§åŠ›åˆåŠ›ä½œç”¨çº¿ä¸åœ°é¢çš„äº¤ç‚¹ã€‚ZMPå¿…é¡»ä¿æŒåœ¨æ”¯æ’‘å¤šè¾¹å½¢å†…ä»¥ç¡®ä¿ç¨³å®šã€‚",
            "keywords": ["ZMP", "é›¶åŠ›çŸ©ç‚¹", "åŠ¨æ€å¹³è¡¡", "æƒ¯æ€§åŠ›", "æ”¯æ’‘"]
        },
        {
            "id": "balance_002",
            "category": "balance",
            "title": "å€’ç«‹æ‘†æ¨¡å‹",
            "content": "åŒè¶³æœºå™¨äººå¯ç®€åŒ–ä¸ºå€’ç«‹æ‘†æ¨¡å‹ã€‚è´¨é‡é›†ä¸­åœ¨é‡å¿ƒï¼Œæ”¯ç‚¹åœ¨è„šè¸ã€‚æ§åˆ¶æ–¹ç¨‹ä¸ºÎ¸''=(g/l)*sin(Î¸)-u/mlÂ²ï¼Œå…¶ä¸­lä¸ºæ‘†é•¿ï¼Œuä¸ºæ§åˆ¶åŠ›çŸ©ã€‚",
            "keywords": ["å€’ç«‹æ‘†", "æ‘†é•¿", "æ§åˆ¶æ–¹ç¨‹", "è„šè¸", "åŠ›çŸ©"]
        },
        {
            "id": "balance_003",
            "category": "balance",
            "title": "PIDå¹³è¡¡æ§åˆ¶",
            "content": "PIDæ§åˆ¶å™¨é€šè¿‡æ¯”ä¾‹(P)ã€ç§¯åˆ†(I)ã€å¾®åˆ†(D)ä¸‰é¡¹æ§åˆ¶è¯¯å·®ã€‚u=Kp*e+Ki*âˆ«e+Kd*de/dtã€‚å¯¹äºå¹³è¡¡æ§åˆ¶ï¼Œå…¸å‹å‚æ•°èŒƒå›´Kp=2-10,Ki=0.1-1,Kd=0.5-5ã€‚",
            "keywords": ["PID", "æ¯”ä¾‹", "ç§¯åˆ†", "å¾®åˆ†", "å‚æ•°", "æ§åˆ¶å™¨"]
        },
        
        # æ­¥æ€è§„åˆ’
        {
            "id": "gait_001",
            "category": "gait",
            "title": "æ­¥æ€å‘¨æœŸ",
            "content": "æ­¥æ€å‘¨æœŸåŒ…æ‹¬æ”¯æ’‘ç›¸å’Œæ‘†åŠ¨ç›¸ã€‚æ”¯æ’‘ç›¸å çº¦60%ï¼Œæ‘†åŠ¨ç›¸çº¦40%ã€‚åŒæ”¯æ’‘æœŸæ˜¯ä¸¤è„šåŒæ—¶ç€åœ°çš„æ—¶é—´ï¼Œçº¦å 20%ï¼Œæ˜¯æœ€ç¨³å®šçš„é˜¶æ®µã€‚",
            "keywords": ["æ­¥æ€", "å‘¨æœŸ", "æ”¯æ’‘ç›¸", "æ‘†åŠ¨ç›¸", "åŒæ”¯æ’‘"]
        },
        {
            "id": "gait_002",
            "category": "gait",
            "title": "æ­¥å¹…ä¸æ­¥é¢‘",
            "content": "æ­¥å¹…(stride length)æ˜¯åŒä¸€åªè„šä¸¤æ¬¡ç€åœ°é—´çš„è·ç¦»ã€‚æ­¥é¢‘(cadence)æ˜¯å•ä½æ—¶é—´æ­¥æ•°ã€‚è¡Œèµ°é€Ÿåº¦=æ­¥å¹…Ã—æ­¥é¢‘/2ã€‚æ­£å¸¸äººæ­¥å¹…çº¦1.2-1.5mï¼Œæ­¥é¢‘çº¦100-120æ­¥/åˆ†ã€‚",
            "keywords": ["æ­¥å¹…", "æ­¥é¢‘", "é€Ÿåº¦", "è·ç¦»", "è¡Œèµ°"]
        },
        
        # å…³èŠ‚æ§åˆ¶
        {
            "id": "joint_001",
            "category": "joint",
            "title": "å…³èŠ‚åŠ›çŸ©",
            "content": "ç”µæœºè¾“å‡ºåŠ›çŸ©Téœ€å…‹æœè´Ÿè½½åŠ›çŸ©ã€‚T=J*Î±+b*Ï‰+Ï„_loadï¼Œå…¶ä¸­Jä¸ºè½¬åŠ¨æƒ¯é‡ï¼ŒÎ±ä¸ºè§’åŠ é€Ÿåº¦ï¼Œbä¸ºé˜»å°¼ç³»æ•°ï¼ŒÏ„_loadä¸ºè´Ÿè½½åŠ›çŸ©ã€‚",
            "keywords": ["åŠ›çŸ©", "ç”µæœº", "è½¬åŠ¨æƒ¯é‡", "è§’åŠ é€Ÿåº¦", "é˜»å°¼"]
        },
        {
            "id": "joint_002",
            "category": "joint",
            "title": "å…³èŠ‚é™ä½",
            "content": "äººä½“é«‹å…³èŠ‚æ´»åŠ¨èŒƒå›´ï¼šå±ˆæ›²0-120Â°ï¼Œä¼¸å±•0-30Â°ï¼Œå¤–å±•0-45Â°ã€‚æœºå™¨äººå…³èŠ‚åº”è®¾ç½®åˆé€‚é™ä½ï¼Œé¿å…è¿‡åº¦è¿åŠ¨é€ æˆæŸåã€‚",
            "keywords": ["é™ä½", "é«‹å…³èŠ‚", "æ´»åŠ¨èŒƒå›´", "å±ˆæ›²", "ä¼¸å±•"]
        },
        
        # æƒ¯æ€§ä¸åŠ¨é‡
        {
            "id": "inertia_001",
            "category": "inertia",
            "title": "è§’åŠ¨é‡å®ˆæ’",
            "content": "æ— å¤–åŠ›çŸ©æ—¶è§’åŠ¨é‡L=IÏ‰å®ˆæ’ã€‚æœºå™¨äººå¯é€šè¿‡æ”¹å˜å§¿æ€ï¼ˆæ”¹å˜è½¬åŠ¨æƒ¯é‡Iï¼‰æ¥è°ƒæ•´è§’é€Ÿåº¦Ï‰ï¼Œç±»ä¼¼æ»‘å†°è¿åŠ¨å‘˜æ”¶ç´§æ‰‹è‡‚åŠ é€Ÿæ—‹è½¬ã€‚",
            "keywords": ["è§’åŠ¨é‡", "å®ˆæ’", "è½¬åŠ¨æƒ¯é‡", "è§’é€Ÿåº¦", "å§¿æ€"]
        },
        {
            "id": "inertia_002",
            "category": "inertia",
            "title": "å†²é‡ä¸åŠ¨é‡",
            "content": "å†²é‡J=F*Î”t=Î”pï¼Œç­‰äºåŠ¨é‡å˜åŒ–ã€‚è½åœ°æ—¶é€šè¿‡å¼¯æ›²è†ç›–å»¶é•¿æ¥è§¦æ—¶é—´ï¼Œå¯å‡å°åœ°é¢å†²å‡»åŠ›ï¼Œä¿æŠ¤å…³èŠ‚ã€‚",
            "keywords": ["å†²é‡", "åŠ¨é‡", "è½åœ°", "è†ç›–", "å†²å‡»åŠ›"]
        },
        
        # èƒ½é‡
        {
            "id": "energy_001",
            "category": "energy",
            "title": "èƒ½é‡æ•ˆç‡",
            "content": "è¡Œèµ°çš„èƒ½é‡æ•ˆç‡ç”¨CoT(Cost of Transport)è¡¡é‡ï¼šCoT=P/(mg*v)ï¼Œå…¶ä¸­Pä¸ºåŠŸç‡ï¼Œvä¸ºé€Ÿåº¦ã€‚äººç±»æ­¥è¡ŒCoTçº¦0.2ï¼Œå¥”è·‘çº¦0.9ã€‚",
            "keywords": ["èƒ½é‡", "æ•ˆç‡", "CoT", "åŠŸç‡", "è¡Œèµ°"]
        },
        {
            "id": "energy_002",
            "category": "energy",
            "title": "åŠ¿èƒ½ä¸åŠ¨èƒ½è½¬æ¢",
            "content": "è¡Œèµ°è¿‡ç¨‹ä¸­åŠ¿èƒ½(mgh)å’ŒåŠ¨èƒ½(0.5mvÂ²)ç›¸äº’è½¬æ¢ã€‚å•è…¿æ”¯æ’‘æ—¶é‡å¿ƒå…ˆå‡é«˜åé™ä½ï¼Œåˆ©ç”¨é‡åŠ›åšåŠŸæé«˜æ•ˆç‡ã€‚",
            "keywords": ["åŠ¿èƒ½", "åŠ¨èƒ½", "è½¬æ¢", "é‡å¿ƒ", "æ•ˆç‡"]
        }
    ]
    
    def __init__(
        self,
        index_path: str = "knowledge/physics_index",
        use_embeddings: bool = True
    ):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            use_embeddings: æ˜¯å¦ä½¿ç”¨åµŒå…¥å‘é‡ï¼ˆéœ€è¦sentence-transformersï¼‰
        """
        self.index_path = Path(index_path)
        self.use_embeddings = use_embeddings
        
        # çŸ¥è¯†åº“
        self.entries: List[KnowledgeEntry] = []
        self.embedder = None
        
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        self._load_or_create_index()
    
    def _load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        index_file = self.index_path / "index.pkl"
        
        if index_file.exists():
            print(f"ğŸ“š åŠ è½½çŸ¥è¯†åº“ç´¢å¼•: {index_file}")
            with open(index_file, 'rb') as f:
                self.entries = pickle.load(f)
        else:
            print("ğŸ“š åˆ›å»ºçŸ¥è¯†åº“ç´¢å¼•...")
            self._create_index()
            self._save_index()
    
    def _create_index(self):
        """åˆ›å»ºç´¢å¼•"""
        # åŠ è½½å†…ç½®çŸ¥è¯†
        for item in self.BUILTIN_KNOWLEDGE:
            entry = KnowledgeEntry(
                id=item['id'],
                category=item['category'],
                title=item['title'],
                content=item['content'],
                keywords=item['keywords']
            )
            self.entries.append(entry)
        
        # å¦‚æœå¯ç”¨åµŒå…¥ï¼Œè®¡ç®—åµŒå…¥å‘é‡
        if self.use_embeddings:
            self._compute_embeddings()
        
        print(f"âœ… çŸ¥è¯†åº“åˆ›å»ºå®Œæˆï¼Œå…± {len(self.entries)} æ¡")
    
    def _compute_embeddings(self):
        """è®¡ç®—åµŒå…¥å‘é‡"""
        try:
            from sentence_transformers import SentenceTransformer
            
            print("æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹...")
            # ä½¿ç”¨å°å‹å¤šè¯­è¨€æ¨¡å‹
            self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            print("æ­£åœ¨è®¡ç®—åµŒå…¥å‘é‡...")
            texts = [f"{e.title}: {e.content}" for e in self.entries]
            embeddings = self.embedder.encode(texts, show_progress_bar=True)
            
            for i, entry in enumerate(self.entries):
                entry.embedding = embeddings[i].tolist()
            
            print("âœ… åµŒå…¥å‘é‡è®¡ç®—å®Œæˆ")
            
        except ImportError:
            print("âš ï¸ sentence-transformersæœªå®‰è£…ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            self.use_embeddings = False
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        self.index_path.mkdir(parents=True, exist_ok=True)
        index_file = self.index_path / "index.pkl"
        
        with open(index_file, 'wb') as f:
            pickle.dump(self.entries, f)
        
        print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {index_file}")
    
    def retrieve(
        self,
        query: str,
        top_k: int = 3,
        category: Optional[str] = None
    ) -> List[Tuple[KnowledgeEntry, float]]:
        """
        æ£€ç´¢ç›¸å…³çŸ¥è¯†
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            category: é™å®šç±»åˆ«
        
        Returns:
            [(çŸ¥è¯†æ¡ç›®, ç›¸å…³åº¦åˆ†æ•°), ...]
        """
        # è¿‡æ»¤ç±»åˆ«
        candidates = self.entries
        if category:
            candidates = [e for e in candidates if e.category == category]
        
        if self.use_embeddings and self.embedder:
            return self._retrieve_by_embedding(query, candidates, top_k)
        else:
            return self._retrieve_by_keyword(query, candidates, top_k)
    
    def _retrieve_by_embedding(
        self,
        query: str,
        candidates: List[KnowledgeEntry],
        top_k: int
    ) -> List[Tuple[KnowledgeEntry, float]]:
        """ä½¿ç”¨åµŒå…¥å‘é‡æ£€ç´¢"""
        import numpy as np
        
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥
        query_emb = self.embedder.encode([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        scores = []
        for entry in candidates:
            if entry.embedding:
                entry_emb = np.array(entry.embedding)
                # ä½™å¼¦ç›¸ä¼¼åº¦
                sim = np.dot(query_emb, entry_emb) / (
                    np.linalg.norm(query_emb) * np.linalg.norm(entry_emb)
                )
                scores.append((entry, float(sim)))
        
        # æ’åºè¿”å›
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def _retrieve_by_keyword(
        self,
        query: str,
        candidates: List[KnowledgeEntry],
        top_k: int
    ) -> List[Tuple[KnowledgeEntry, float]]:
        """ä½¿ç”¨å…³é”®è¯åŒ¹é…æ£€ç´¢"""
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        scores = []
        for entry in candidates:
            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = 0
            for kw in entry.keywords:
                if kw.lower() in query_lower:
                    keyword_score += 2
                elif any(kw.lower() in w for w in query_words):
                    keyword_score += 1
            
            # æ ‡é¢˜åŒ¹é…
            if any(w in entry.title.lower() for w in query_words):
                keyword_score += 1
            
            # å†…å®¹åŒ¹é…
            content_matches = sum(1 for w in query_words if w in entry.content.lower())
            keyword_score += content_matches * 0.5
            
            if keyword_score > 0:
                # å½’ä¸€åŒ–åˆ†æ•°åˆ°0-1
                normalized = min(1.0, keyword_score / 10.0)
                scores.append((entry, normalized))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def augment_prompt(
        self,
        base_prompt: str,
        sensor_data: dict,
        max_context_length: int = 500
    ) -> str:
        """
        ç”¨RAGæ£€ç´¢ç»“æœå¢å¼ºPrompt
        
        Args:
            base_prompt: åŸºç¡€Prompt
            sensor_data: ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆç”¨äºæå–æŸ¥è¯¢ï¼‰
            max_context_length: ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦
        
        Returns:
            å¢å¼ºåçš„Prompt
        """
        # ä»ä¼ æ„Ÿå™¨æ•°æ®æå–æŸ¥è¯¢
        query = self._extract_query(sensor_data)
        
        # æ£€ç´¢ç›¸å…³çŸ¥è¯†
        results = self.retrieve(query, top_k=3)
        
        if not results:
            return base_prompt
        
        # æ„å»ºçŸ¥è¯†ä¸Šä¸‹æ–‡
        context_parts = []
        total_length = 0
        
        for entry, score in results:
            if score < 0.3:  # ç›¸å…³åº¦é˜ˆå€¼
                continue
            
            snippet = f"ã€{entry.title}ã€‘{entry.content}"
            
            if total_length + len(snippet) > max_context_length:
                break
            
            context_parts.append(snippet)
            total_length += len(snippet)
        
        if not context_parts:
            return base_prompt
        
        context = "\n".join(context_parts)
        
        return f"""{base_prompt}

## ç›¸å…³ç‰©ç†çŸ¥è¯†
{context}

è¯·ç»“åˆä»¥ä¸Šç‰©ç†çŸ¥è¯†è¿›è¡Œåˆ†æå’Œå†³ç­–ã€‚"""
    
    def _extract_query(self, sensor_data: dict) -> str:
        """ä»ä¼ æ„Ÿå™¨æ•°æ®æå–æŸ¥è¯¢å…³é”®è¯"""
        queries = []
        
        orient = sensor_data.get('sensors', {}).get('imu', {}).get('orient', [0, 0, 0])
        height = sensor_data.get('torso_height', 1.0)
        
        # æ ¹æ®çŠ¶æ€æ·»åŠ æŸ¥è¯¢å…³é”®è¯
        roll, pitch = orient[0], orient[1]
        
        if abs(roll) > 10 or abs(pitch) > 10:
            queries.append("å¹³è¡¡æ§åˆ¶ å€¾æ–œ æ¢å¤")
        
        if height < 0.5:
            queries.append("é‡å¿ƒ ç¨³å®šæ€§ è·Œå€’")
        
        if abs(roll) > 30 or abs(pitch) > 30:
            queries.append("å€¾è¦† åŠ›çŸ© ç´§æ€¥")
        
        # é»˜è®¤æŸ¥è¯¢
        if not queries:
            queries.append("å¹³è¡¡ ç«™ç«‹ æ§åˆ¶")
        
        return " ".join(queries)
    
    def add_knowledge(self, entry: dict):
        """æ·»åŠ æ–°çŸ¥è¯†æ¡ç›®"""
        new_entry = KnowledgeEntry(
            id=entry.get('id', f"custom_{len(self.entries)}"),
            category=entry.get('category', 'custom'),
            title=entry['title'],
            content=entry['content'],
            keywords=entry.get('keywords', [])
        )
        
        # è®¡ç®—åµŒå…¥
        if self.use_embeddings and self.embedder:
            text = f"{new_entry.title}: {new_entry.content}"
            embedding = self.embedder.encode([text])[0]
            new_entry.embedding = embedding.tolist()
        
        self.entries.append(new_entry)
        self._save_index()
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        categories = {}
        for entry in self.entries:
            categories[entry.category] = categories.get(entry.category, 0) + 1
        
        return {
            "total_entries": len(self.entries),
            "categories": categories,
            "embeddings_enabled": self.use_embeddings,
            "index_path": str(self.index_path)
        }


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("RAGç‰©ç†çŸ¥è¯†åº“æµ‹è¯•\n")
    
    # åˆ›å»ºçŸ¥è¯†åº“ï¼ˆç¦»çº¿æ¨¡å¼ï¼Œä¸ä½¿ç”¨åµŒå…¥ï¼‰
    kb = PhysicsKnowledgeBase(
        index_path="d:/æ–°å»ºæ–‡ä»¶å¤¹/AGI-Walker/knowledge/physics_index",
        use_embeddings=False  # ç¦»çº¿æ¨¡å¼
    )
    
    # æµ‹è¯•æ£€ç´¢
    print("\n=== æµ‹è¯•æ£€ç´¢ ===")
    queries = [
        "æœºå™¨äººå€¾æ–œæ€ä¹ˆåŠ",
        "å¦‚ä½•ä¿æŒå¹³è¡¡",
        "PIDå‚æ•°è°ƒèŠ‚",
        "æ­¥æ€è§„åˆ’"
    ]
    
    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = kb.retrieve(query, top_k=2)
        for entry, score in results:
            print(f"  - [{score:.2f}] {entry.title}")
    
    # æµ‹è¯•Promptå¢å¼º
    print("\n=== æµ‹è¯•Promptå¢å¼º ===")
    sensor_data = {
        "sensors": {
            "imu": {"orient": [15.0, -8.0, 0.0]},
            "joints": {"hip_left": {"angle": 5.0}, "hip_right": {"angle": -3.0}}
        },
        "torso_height": 1.2
    }
    
    base_prompt = "è¯·åˆ†æå½“å‰æœºå™¨äººçŠ¶æ€å¹¶æä¾›æ§åˆ¶å»ºè®®"
    enhanced = kb.augment_prompt(base_prompt, sensor_data)
    print(enhanced[:500] + "...")
    
    # ç»Ÿè®¡
    print("\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
    stats = kb.get_stats()
    print(json.dumps(stats, indent=2, ensure_ascii=False))
