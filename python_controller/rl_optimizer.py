"""
å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–å™¨
åŸºäºStable-Baselines3ï¼Œé›†æˆGodotä»¿çœŸç¯å¢ƒ
"""

import os
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Union, Callable
from dataclasses import dataclass
import numpy as np

# å»¶è¿Ÿå¯¼å…¥SB3
sb3 = None
VecEnv = None


def _init_sb3():
    """å»¶è¿Ÿåˆå§‹åŒ–Stable-Baselines3"""
    global sb3, VecEnv
    if sb3 is None:
        try:
            import stable_baselines3 as _sb3
            from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
            sb3 = _sb3
            VecEnv = DummyVecEnv
            print(f"âœ… Stable-Baselines3 v{sb3.__version__} å·²åŠ è½½")
            return True
        except ImportError:
            print("âš ï¸ Stable-Baselines3æœªå®‰è£…")
            print("è¯·è¿è¡Œ: pip install stable-baselines3")
            return False
    return True


@dataclass
class RLConfig:
    """å¼ºåŒ–å­¦ä¹ é…ç½®"""
    algorithm: str = "PPO"          # ç®—æ³•é€‰æ‹©
    learning_rate: float = 3e-4     # å­¦ä¹ ç‡
    n_steps: int = 2048             # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
    batch_size: int = 64            # æ‰¹é‡å¤§å°
    n_epochs: int = 10              # æ¯æ¬¡æ›´æ–°çš„epochs
    gamma: float = 0.99             # æŠ˜æ‰£å› å­
    gae_lambda: float = 0.95        # GAE lambda
    clip_range: float = 0.2         # PPO clipèŒƒå›´
    ent_coef: float = 0.01          # ç†µç³»æ•°
    verbose: int = 1                # æ—¥å¿—çº§åˆ«
    tensorboard_log: str = "./tensorboard_logs"


class RLOptimizer:
    """
    å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–å™¨
    
    æ”¯æŒç®—æ³•ï¼šPPO, SAC, TD3, A2C
    é›†æˆGodotä»¿çœŸç¯å¢ƒè¿›è¡Œè‡ªåŠ¨åŒ–ä¼˜åŒ–
    """
    
    ALGORITHMS = {
        "PPO": "stable_baselines3.PPO",
        "SAC": "stable_baselines3.SAC",
        "TD3": "stable_baselines3.TD3",
        "A2C": "stable_baselines3.A2C"
    }
    
    def __init__(
        self,
        env,
        config: Optional[RLConfig] = None,
        save_dir: str = "d:/æ–°å»ºæ–‡ä»¶å¤¹/AGI-Walker/models/rl"
    ):
        """
        åˆå§‹åŒ–RLä¼˜åŒ–å™¨
        
        Args:
            env: Gymç¯å¢ƒï¼ˆGodotRobotEnvï¼‰
            config: RLé…ç½®
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        if not _init_sb3():
            raise ImportError("Stable-Baselines3ä¸å¯ç”¨")
        
        self.config = config or RLConfig()
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # åŒ…è£…ç¯å¢ƒ
        self.env = env
        self.vec_env = VecEnv([lambda: env]) if VecEnv else None
        
        # åˆ›å»ºæ¨¡å‹
        self.model = self._create_model()
        
        # è®­ç»ƒå†å²
        self.training_history = []
        self.best_reward = float('-inf')
        
        print(f"âœ… RLä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç®—æ³•: {self.config.algorithm}")
        print(f"   ä¿å­˜ç›®å½•: {self.save_dir}")
    
    def _create_model(self):
        """åˆ›å»ºRLæ¨¡å‹"""
        algorithm = self.config.algorithm.upper()
        
        if algorithm not in self.ALGORITHMS:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        
        # åŠ¨æ€å¯¼å…¥ç®—æ³•
        if algorithm == "PPO":
            model = sb3.PPO(
                "MlpPolicy",
                self.vec_env or self.env,
                learning_rate=self.config.learning_rate,
                n_steps=self.config.n_steps,
                batch_size=self.config.batch_size,
                n_epochs=self.config.n_epochs,
                gamma=self.config.gamma,
                gae_lambda=self.config.gae_lambda,
                clip_range=self.config.clip_range,
                ent_coef=self.config.ent_coef,
                verbose=self.config.verbose,
                tensorboard_log=self.config.tensorboard_log
            )
        elif algorithm == "SAC":
            model = sb3.SAC(
                "MlpPolicy",
                self.vec_env or self.env,
                learning_rate=self.config.learning_rate,
                batch_size=self.config.batch_size,
                gamma=self.config.gamma,
                verbose=self.config.verbose,
                tensorboard_log=self.config.tensorboard_log
            )
        elif algorithm == "TD3":
            model = sb3.TD3(
                "MlpPolicy",
                self.vec_env or self.env,
                learning_rate=self.config.learning_rate,
                batch_size=self.config.batch_size,
                gamma=self.config.gamma,
                verbose=self.config.verbose,
                tensorboard_log=self.config.tensorboard_log
            )
        elif algorithm == "A2C":
            model = sb3.A2C(
                "MlpPolicy",
                self.vec_env or self.env,
                learning_rate=self.config.learning_rate,
                n_steps=self.config.n_steps,
                gamma=self.config.gamma,
                gae_lambda=self.config.gae_lambda,
                ent_coef=self.config.ent_coef,
                verbose=self.config.verbose,
                tensorboard_log=self.config.tensorboard_log
            )
        
        return model
    
    def train(
        self,
        total_timesteps: int = 100000,
        eval_freq: int = 10000,
        n_eval_episodes: int = 5,
        callback: Optional[Callable] = None
    ) -> dict:
        """
        è®­ç»ƒRLä»£ç†
        
        Args:
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            eval_freq: è¯„ä¼°é¢‘ç‡
            n_eval_episodes: æ¯æ¬¡è¯„ä¼°çš„episodeæ•°
            callback: è‡ªå®šä¹‰å›è°ƒ
        
        Returns:
            è®­ç»ƒç»“æœç»Ÿè®¡
        """
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"   æ€»æ­¥æ•°: {total_timesteps}")
        print(f"   è¯„ä¼°é¢‘ç‡: {eval_freq}")
        
        start_time = time.time()
        
        # åˆ›å»ºå›è°ƒ
        from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
        
        callbacks = []
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=eval_freq,
            save_path=str(self.save_dir / "checkpoints"),
            name_prefix=f"{self.config.algorithm.lower()}_model"
        )
        callbacks.append(checkpoint_callback)
        
        # è¯„ä¼°å›è°ƒ
        if self.vec_env:
            eval_callback = EvalCallback(
                self.vec_env,
                best_model_save_path=str(self.save_dir / "best_model"),
                log_path=str(self.save_dir / "eval_logs"),
                eval_freq=eval_freq,
                n_eval_episodes=n_eval_episodes,
                deterministic=True
            )
            callbacks.append(eval_callback)
        
        if callback:
            callbacks.append(callback)
        
        # è®­ç»ƒ
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks if callbacks else None,
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.save_dir / f"{self.config.algorithm.lower()}_final.zip"
        self.model.save(str(final_path))
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {final_path}")
        
        # è®°å½•å†å²
        result = {
            "algorithm": self.config.algorithm,
            "total_timesteps": total_timesteps,
            "training_time": training_time,
            "model_path": str(final_path)
        }
        self.training_history.append(result)
        
        return result
    
    def evaluate(self, n_episodes: int = 10) -> dict:
        """è¯„ä¼°å½“å‰ç­–ç•¥"""
        from stable_baselines3.common.evaluation import evaluate_policy
        
        print(f"\nğŸ“Š è¯„ä¼°ç­–ç•¥ ({n_episodes} episodes)")
        
        mean_reward, std_reward = evaluate_policy(
            self.model,
            self.vec_env or self.env,
            n_eval_episodes=n_episodes,
            deterministic=True
        )
        
        result = {
            "mean_reward": mean_reward,
            "std_reward": std_reward,
            "n_episodes": n_episodes
        }
        
        print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
        
        return result
    
    def load(self, path: str):
        """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹"""
        algorithm = self.config.algorithm.upper()
        
        if algorithm == "PPO":
            self.model = sb3.PPO.load(path, env=self.vec_env or self.env)
        elif algorithm == "SAC":
            self.model = sb3.SAC.load(path, env=self.vec_env or self.env)
        elif algorithm == "TD3":
            self.model = sb3.TD3.load(path, env=self.vec_env or self.env)
        elif algorithm == "A2C":
            self.model = sb3.A2C.load(path, env=self.vec_env or self.env)
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {path}")
    
    def export_policy_onnx(self, output_path: str):
        """å¯¼å‡ºç­–ç•¥ä¸ºONNXæ ¼å¼"""
        try:
            import torch
            
            # è·å–ç­–ç•¥ç½‘ç»œ
            policy = self.model.policy
            
            # åˆ›å»ºdummyè¾“å…¥
            obs_shape = self.env.observation_space.shape
            dummy_obs = torch.zeros((1,) + obs_shape)
            
            # å¯¼å‡º
            torch.onnx.export(
                policy,
                dummy_obs,
                output_path,
                opset_version=11,
                input_names=["observation"],
                output_names=["action"]
            )
            
            print(f"âœ… ç­–ç•¥å·²å¯¼å‡ºä¸ºONNX: {output_path}")
            
        except Exception as e:
            print(f"âŒ ONNXå¯¼å‡ºå¤±è´¥: {e}")
    
    def get_action(self, observation: np.ndarray, deterministic: bool = True) -> np.ndarray:
        """è·å–åŠ¨ä½œ"""
        action, _ = self.model.predict(observation, deterministic=deterministic)
        return action
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "algorithm": self.config.algorithm,
            "training_history": self.training_history,
            "model_path": str(self.save_dir)
        }


class DummyEnv:
    """è™šæ‹Ÿç¯å¢ƒï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    
    def __init__(self):
        import gymnasium as gym
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(12,))
        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(2,))
        self._step_count = 0
    
    def reset(self, seed=None, options=None):
        self._step_count = 0
        return np.zeros(12, dtype=np.float32), {}
    
    def step(self, action):
        self._step_count += 1
        obs = np.random.randn(12).astype(np.float32) * 0.1
        reward = 1.0 - np.abs(obs[0])  # ç®€å•å¥–åŠ±
        terminated = abs(obs[0]) > 2.0
        truncated = self._step_count > 1000
        return obs, reward, terminated, truncated, {}
    
    def close(self):
        pass


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RLç­–ç•¥ä¼˜åŒ–å™¨")
    
    parser.add_argument("--algorithm", default="PPO",
                        choices=["PPO", "SAC", "TD3", "A2C"],
                        help="RLç®—æ³•")
    parser.add_argument("--timesteps", type=int, default=100000,
                        help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--eval-freq", type=int, default=10000,
                        help="è¯„ä¼°é¢‘ç‡")
    parser.add_argument("--learning-rate", type=float, default=3e-4,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--use-godot", action="store_true",
                        help="ä½¿ç”¨Godotç¯å¢ƒï¼ˆé»˜è®¤ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¯å¢ƒ
    if args.use_godot:
        import sys
        sys.path.insert(0, '../python_api/godot_robot_env')
        from gym_env import GodotRobotEnv
        env = GodotRobotEnv()
    else:
        print("ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒè¿›è¡Œæµ‹è¯•...")
        env = DummyEnv()
    
    # åˆ›å»ºé…ç½®
    config = RLConfig(
        algorithm=args.algorithm,
        learning_rate=args.learning_rate
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = RLOptimizer(env, config)
    
    # è®­ç»ƒ
    result = optimizer.train(
        total_timesteps=args.timesteps,
        eval_freq=args.eval_freq
    )
    
    # è¯„ä¼°
    eval_result = optimizer.evaluate(n_episodes=5)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 50)
    print("è®­ç»ƒå®Œæˆ")
    print("=" * 50)
    print(json.dumps(result, indent=2))
    print(json.dumps(eval_result, indent=2))
    
    env.close()


if __name__ == "__main__":
    main()
