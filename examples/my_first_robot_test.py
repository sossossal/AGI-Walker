"""
æµ‹è¯•è®­ç»ƒå¥½çš„æœºå™¨äºº
è§‚å¯Ÿæœºå™¨äººçš„å®é™…è¡¨ç°
"""

import gymnasium as gym
from stable_baselines3 import PPO
import time

print("="*70)
print("  æµ‹è¯•æˆ‘çš„æœºå™¨äºº")
print("="*70)

# ============ åŠ è½½æ¨¡å‹ ============
print("\n[1/3] åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
try:
    model = PPO.load("my_robot_models/my_robot_final")
    print("  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except FileNotFoundError:
    print("  âœ— é”™è¯¯: æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
    print("  è¯·å…ˆè¿è¡Œ 'python my_first_robot_train.py' è®­ç»ƒæ¨¡å‹")
    exit(1)

# ============ åˆ›å»ºç¯å¢ƒ ============
print("\n[2/3] åˆ›å»ºæµ‹è¯•ç¯å¢ƒ...")
print("  æ¨¡å¼: äººç±»å¯è§†åŒ– (å¦‚æœæ”¯æŒ)")

try:
    env = gym.make('AGI-Walker/Walker2D-v0', render_mode="human")
except:
    # å¦‚æœä¸æ”¯æŒæ¸²æŸ“ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼
    env = gym.make('AGI-Walker/Walker2D-v0')
    print("  æ³¨æ„: å½“å‰ç¯å¢ƒä¸æ”¯æŒå¯è§†åŒ–æ¸²æŸ“")

print("  âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")

# ============ æµ‹è¯•å›åˆ ============
print("\n[3/3] å¼€å§‹æµ‹è¯•...")
print("-"*70)

n_episodes = 5
episode_rewards = []
episode_lengths = []

for episode in range(n_episodes):
    obs, _ = env.reset()
    done = False
    episode_reward = 0
    step_count = 0
    
    print(f"\nå›åˆ {episode + 1}/{n_episodes}:")
    print("  å‰è¿›ä¸­", end="", flush=True)
    
    while not done and step_count < 1000:
        # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥é¢„æµ‹åŠ¨ä½œ
        action, _states = model.predict(obs, deterministic=True)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        episode_reward += reward
        step_count += 1
        
        # æ¯100æ­¥æ˜¾ç¤ºä¸€ä¸ªç‚¹
        if step_count % 100 == 0:
            print(".", end="", flush=True)
        
        # å‡é€Ÿä»¥ä¾¿è§‚å¯Ÿï¼ˆå¦‚æœæœ‰æ¸²æŸ“ï¼‰
        time.sleep(0.01)
    
    episode_rewards.append(episode_reward)
    episode_lengths.append(step_count)
    
    print()
    print(f"  ç»“æœ:")
    print(f"    - æ­¥æ•°: {step_count}")
    print(f"    - å¥–åŠ±: {episode_reward:.2f}")
    
    if step_count >= 1000:
        print(f"    - çŠ¶æ€: âœ“ æˆåŠŸå®Œæˆ1000æ­¥ï¼")
    else:
        print(f"    - çŠ¶æ€: æå‰ç»ˆæ­¢")

env.close()

# ============ ç»Ÿè®¡ç»“æœ ============
import numpy as np

print("\n"+"="*70)
print("  æµ‹è¯•ç»“æœç»Ÿè®¡")
print("="*70)
print(f"  å›åˆæ•°: {n_episodes}")
print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
print(f"  å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f}")
print(f"  æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
print(f"  æœ€ä½å¥–åŠ±: {np.min(episode_rewards):.2f}")
print("-"*70)

# æ€§èƒ½è¯„ä¼°
avg_reward = np.mean(episode_rewards)
if avg_reward > 200:
    rating = "ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€ï¼"
    comment = "æœºå™¨äººå·²ç»å­¦ä¼šç¨³å®šè¡Œèµ°"
elif avg_reward > 100:
    rating = "â­â­ è‰¯å¥½"
    comment = "æœºå™¨äººè¡¨ç°ä¸é”™ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒæå‡"
elif avg_reward > 50:
    rating = "â­ åŠæ ¼"
    comment = "æœºå™¨äººåˆæ­¥æŒæ¡äº†å¹³è¡¡ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ—¶é—´"
else:
    rating = "éœ€æ”¹è¿›"
    comment = "å»ºè®®é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°"

print(f"\n  æ€§èƒ½è¯„çº§: {rating}")
print(f"  è¯„ä»·: {comment}")
print("="*70)

# å»ºè®®
print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
if avg_reward < 150:
    print("  1. å¢åŠ è®­ç»ƒæ­¥æ•° (ä¾‹å¦‚: 200,000 æˆ– 500,000)")
    print("  2. å°è¯•è°ƒæ•´å­¦ä¹ ç‡")
    print("  3. ä½¿ç”¨ç¦»çº¿RLæˆ–æ¨¡ä»¿å­¦ä¹ åŠ é€Ÿè®­ç»ƒ")
else:
    print("  âœ“ å½“å‰æ€§èƒ½å·²ç»å¾ˆå¥½!")
    print("  å¯ä»¥å°è¯•:")
    print("    - å››è¶³æœºå™¨äºº: examples/quadruped_training.py")
    print("    - æ›´å¤æ‚çš„ç¯å¢ƒ")
    print("    - éƒ¨ç½²åˆ°çœŸå®ç¡¬ä»¶")

print("\næŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
print("  tensorboard --logdir=./my_robot_logs/")
print("  è®¿é—®: http://localhost:6006")
print("="*70)
