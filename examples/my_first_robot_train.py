"""
æˆ‘çš„ç¬¬ä¸€ä¸ªæœºå™¨äººè®­ç»ƒé¡¹ç›®
è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¯è¿è¡Œç¤ºä¾‹ï¼Œå±•ç¤ºä»é›¶å¼€å§‹åˆ›å»ºå’Œè®­ç»ƒæœºå™¨äººçš„å…¨è¿‡ç¨‹
"""

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
import os
import numpy as np

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("my_robot_models", exist_ok=True)
os.makedirs("my_robot_logs", exist_ok=True)

print("="*70)
print("  æ¬¢è¿æ¥åˆ° AGI-Walkerï¼")
print("  æˆ‘çš„ç¬¬ä¸€ä¸ªæœºå™¨äººè®­ç»ƒé¡¹ç›®")
print("="*70)

# ============ ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºç¯å¢ƒ ============
print("\n[æ­¥éª¤ 1/5] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
print("  ç¯å¢ƒç±»å‹: Walker2D (åŒè¶³è¡Œèµ°)")
print("  è§‚æµ‹ç»´åº¦: 17ç»´ (å…³èŠ‚è§’åº¦ã€é€Ÿåº¦ã€èº«ä½“çŠ¶æ€)")
print("  åŠ¨ä½œç»´åº¦: 6ç»´ (6ä¸ªå…³èŠ‚çš„æ‰­çŸ©æ§åˆ¶)")

env = DummyVecEnv([lambda: gym.make('AGI-Walker/Walker2D-v0')])
eval_env = DummyVecEnv([lambda: gym.make('AGI-Walker/Walker2D-v0')])

print("  âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")

# ============ ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæ¨¡å‹ ============
print("\n[æ­¥éª¤ 2/5] åˆ›å»º PPO å¼ºåŒ–å­¦ä¹ æ¨¡å‹...")
print("  ç®—æ³•: PPO (Proximal Policy Optimization)")
print("  ç­–ç•¥ç½‘ç»œ: å¤šå±‚æ„ŸçŸ¥æœº (64-64)")
print("  å­¦ä¹ ç‡: 3e-4")

model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,           # æ¯æ¬¡æ”¶é›†2048æ­¥ç»éªŒ
    batch_size=64,          # æ¯æ‰¹64ä¸ªæ ·æœ¬
    n_epochs=10,            # æ¯æ¬¡æ›´æ–°è®­ç»ƒ10è½®
    gamma=0.99,             # æŠ˜æ‰£å› å­
    gae_lambda=0.95,        # GAEå‚æ•°
    clip_range=0.2,         # PPOå‰ªåˆ‡èŒƒå›´
    verbose=1,              # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    tensorboard_log="./my_robot_logs/"
)

print("  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
print(f"  ç­–ç•¥å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.policy.parameters()):,}")

# ============ ç¬¬ä¸‰æ­¥ï¼šè®¾ç½®å›è°ƒ ============
print("\n[æ­¥éª¤ 3/5] è®¾ç½®è®­ç»ƒå›è°ƒ...")

# å®šæœŸä¿å­˜æ¨¡å‹
checkpoint_callback = CheckpointCallback(
    save_freq=10000,
    save_path='./my_robot_models/',
    name_prefix='checkpoint'
)

# å®šæœŸè¯„ä¼°
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path='./my_robot_models/',
    log_path='./my_robot_logs/',
    eval_freq=5000,
    n_eval_episodes=5,
    deterministic=True,
    render=False
)

print("  âœ“ å›è°ƒè®¾ç½®å®Œæˆï¼")
print("    - æ¯ 10,000 æ­¥ä¿å­˜æ£€æŸ¥ç‚¹")
print("    - æ¯ 5,000 æ­¥è¯„ä¼°ä¸€æ¬¡")

# ============ ç¬¬å››æ­¥ï¼šå¼€å§‹è®­ç»ƒ ============
print("\n[æ­¥éª¤ 4/5] å¼€å§‹è®­ç»ƒ...")
print("-"*70)
print("  è®­ç»ƒå‚æ•°:")
print(f"    æ€»è®­ç»ƒæ­¥æ•°: 100,000")
print(f"    é¢„è®¡æ—¶é—´: 10-15åˆ†é’Ÿ (å–å†³äºç¡¬ä»¶)")
print(f"    æ—¥å¿—ç›®å½•: ./my_robot_logs/")
print(f"    æ¨¡å‹ä¿å­˜: ./my_robot_models/")
print("-"*70)
print("\n  ğŸ’¡ æç¤º:")
print("    - è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥æŒ‰ Ctrl+C æš‚åœ")
print("    - åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ 'tensorboard --logdir=./my_robot_logs/' å®æ—¶æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
print(f"    - ç„¶åè®¿é—® http://localhost:6006\n")

input("æŒ‰å›è½¦é”®å¼€å§‹è®­ç»ƒ...")

try:
    model.learn(
        total_timesteps=100_000,
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True
    )
    
    print("\n"+"="*70)
    print("  âœ“ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    
except KeyboardInterrupt:
    print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    print("å·²ä¿å­˜çš„æ£€æŸ¥ç‚¹å¯ä»¥ç”¨äºç»§ç»­è®­ç»ƒ")

# ============ ç¬¬äº”æ­¥ï¼šä¿å­˜å’Œæµ‹è¯• ============
print("\n[æ­¥éª¤ 5/5] ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
model.save("my_robot_models/my_robot_final")
print("  âœ“ æ¨¡å‹å·²ä¿å­˜: my_robot_models/my_robot_final.zip")

# å¿«é€Ÿæµ‹è¯•
print("\næ­£åœ¨è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
test_env = gym.make('AGI-Walker/Walker2D-v0')
obs, _ = test_env.reset()
total_reward = 0
steps = 0

for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, _ = test_env.step(action)
    total_reward += reward
    steps += 1
    if done or truncated:
        break

test_env.close()

print(f"  æµ‹è¯•ç»“æœ:")
print(f"    - å­˜æ´»æ­¥æ•°: {steps}")
print(f"    - ç´¯è®¡å¥–åŠ±: {total_reward:.2f}")

# ============ å®Œæˆæ€»ç»“ ============
print("\n"+"="*70)
print("  ğŸ‰ æ­å–œï¼ä½ å·²ç»æˆåŠŸè®­ç»ƒäº†ç¬¬ä¸€ä¸ªæœºå™¨äººï¼")
print("="*70)

print("\nä¸‹ä¸€æ­¥:")
print("  1. è¿è¡Œæµ‹è¯•è„šæœ¬:")
print("     python my_first_robot_test.py")
print()
print("  2. æŸ¥çœ‹è®­ç»ƒæ›²çº¿:")
print("     tensorboard --logdir=./my_robot_logs/")
print("     ç„¶åè®¿é—®: http://localhost:6006")
print()
print("  3. å°è¯•æ›´å¤šåŠŸèƒ½:")
print("     - examples/quadruped_training.py (å››è¶³æœºå™¨äºº)")
print("     - examples/offline_rl_demo.py (ç¦»çº¿å¼ºåŒ–å­¦ä¹ )")
print("     - examples/imitation_learning_demo.py (æ¨¡ä»¿å­¦ä¹ )")
print()
print("  ğŸ“š å­¦ä¹ èµ„æº:")
print("     - å®Œæ•´æ•™ç¨‹: GETTING_STARTED.md")
print("     - API æ–‡æ¡£: docs/")
print("     - GitHub: https://github.com/sossossal/AGI-Walker")
print("="*70)
