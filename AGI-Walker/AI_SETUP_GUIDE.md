# AGI-Walker AIæ¨¡å‹é›†æˆå¿«é€ŸæŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å¿«é€Ÿå®ŒæˆAIæ¨¡å‹çš„å®‰è£…å’Œé…ç½®ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿï¼‰

### æ­¥éª¤1: å®‰è£…Ollama

**Windows**:
```powershell
winget install Ollama.Ollama
```

**æˆ–ä¸‹è½½å®‰è£…å™¨**: https://ollama.com/download

### æ­¥éª¤2: ä¸‹è½½AIæ¨¡å‹

```bash
ollama pull phi3:mini
```

ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆçº¦2.3GBï¼‰

### æ­¥éª¤3: å®‰è£…Pythonä¾èµ–

```bash
cd python_controller
pip install ollama
```

### æ­¥éª¤4: æµ‹è¯•AIæ¨¡å‹

```bash
python ai_model.py
```

åº”è¯¥çœ‹åˆ°:
```
âœ… æ¨¡å‹ phi3:mini å·²åŠ è½½
æ‰§è¡Œæ¨ç†...

é¢„æµ‹åŠ¨ä½œ:
{
  "motors": {
    "hip_left": -2.3,
    "hip_right": 1.8
  },
  "confidence": 0.92
}
```

### æ­¥éª¤5: è¿è¡ŒAIæ§åˆ¶å™¨

1. **å¯åŠ¨Godotä»¿çœŸ**ï¼ˆæŒ‰F5ï¼‰
2. **è¿è¡ŒAIæ§åˆ¶å™¨**:
   ```bash
   python ai_controller.py --duration 60
   ```

---

## ğŸ“‹ è¯¦ç»†å®‰è£…æ­¥éª¤

### æ–¹æ¡ˆA: Ollamaï¼ˆæ¨èï¼‰

#### 1. å®‰è£…Ollama

**Windows PowerShell**:
```powershell
# æ–¹æ³•1: ä½¿ç”¨winget
winget install Ollama.Ollama

# æ–¹æ³•2: ä½¿ç”¨Chocolatey
choco install ollama

# æ–¹æ³•3: æ‰‹åŠ¨ä¸‹è½½
# è®¿é—® https://ollama.com/download
```

å®‰è£…åï¼ŒOllamaä¼šè‡ªåŠ¨å¯åŠ¨ä¸ºåå°æœåŠ¡ã€‚

#### 2. éªŒè¯å®‰è£…

```bash
ollama --version
```

åº”è¯¥æ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œå¦‚: `ollama version 0.1.x`

#### 3. ä¸‹è½½æ¨¡å‹

**Phi-3-mini**ï¼ˆæ¨èï¼‰:
```bash
ollama pull phi3:mini
```

**å…¶ä»–é€‰é¡¹**:
```bash
ollama pull gemma:2b      # æ›´å°æ›´å¿«
ollama pull qwen2:3b      # ä¸­æ–‡ä¼˜åŒ–
```

#### 4. æµ‹è¯•æ¨¡å‹

```bash
ollama run phi3:mini
```

è¾“å…¥: `ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±`

å¦‚æœæ¨¡å‹æ­£å¸¸å“åº”ï¼Œè¯´æ˜å®‰è£…æˆåŠŸï¼

#### 5. å®‰è£…Pythonå®¢æˆ·ç«¯

```bash
pip install ollama
```

---

### æ–¹æ¡ˆB: llama.cppï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

#### 1. ç¼–è¯‘llama.cpp

**Windows (ä½¿ç”¨CMake)**:
```powershell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# ç¼–è¯‘
cmake -B build
cmake --build build --config Release
```

**å¯ç”¨GPUåŠ é€Ÿ** (å¦‚æœæœ‰NVIDIA GPU):
```powershell
cmake -B build -DLLAMA_CUDA=ON
cmake --build build --config Release
```

#### 2. ä¸‹è½½æ¨¡å‹

ä»HuggingFaceä¸‹è½½GGUFæ ¼å¼:

```bash
# ä½¿ç”¨huggingface-cli
pip install huggingface-hub

huggingface-cli download \
  microsoft/Phi-3-mini-4k-instruct-gguf \
  Phi-3-mini-4k-instruct-q4.gguf \
  --local-dir ./models
```

**æˆ–æ‰‹åŠ¨ä¸‹è½½**:
https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf

#### 3. æµ‹è¯•æ¨¡å‹

```bash
.\build\bin\Release\main.exe \
  -m models/Phi-3-mini-4k-instruct-q4.gguf \
  -p "Hello, introduce yourself" \
  -n 50
```

#### 4. å®‰è£…Pythonç»‘å®š

```bash
pip install llama-cpp-python
```

**GPUç‰ˆæœ¬**:
```bash
CMAKE_ARGS="-DLLAMA_CUDA=ON" pip install llama-cpp-python
```

---

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### æµ‹è¯•1: æ¨¡å‹å“åº”é€Ÿåº¦

```bash
python -c "
from ai_model import create_ai_model
import time

ai = create_ai_model()
dummy = {
    'sensors': {
        'imu': {'orient': [5, -3, 0]},
        'joints': {
            'hip_left': {'angle': 10},
            'hip_right': {'angle': -8}
        }
    },
    'torso_height': 1.45
}

# æµ‹è¯•10æ¬¡
times = []
for _ in range(10):
    t0 = time.time()
    ai.predict(dummy)
    times.append(time.time() - t0)

print(f'å¹³å‡: {sum(times)/len(times)*1000:.1f}ms')
"
```

**ç›®æ ‡**: < 100ms

### æµ‹è¯•2: JSONæ ¼å¼

```bash
python ai_model.py
```

æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºæœ‰æ•ˆJSONã€‚

### æµ‹è¯•3: é›†æˆæ§åˆ¶

```bash
# 1. å¯åŠ¨Godot (å¦ä¸€ä¸ªç»ˆç«¯)
# 2. è¿è¡ŒAIæ§åˆ¶å™¨
python ai_controller.py --duration 30
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ— è¿æ¥é”™è¯¯
- âœ… æ§åˆ¶é¢‘ç‡ > 20Hz
- âœ… æ— JSONè§£æé”™è¯¯
- âœ… æœºå™¨äººä¿æŒç«™ç«‹

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### é—®é¢˜1: OllamaæœåŠ¡æœªå¯åŠ¨

**ç—‡çŠ¶**:
```
âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡
```

**è§£å†³**:
```bash
# Windows: åœ¨å¼€å§‹èœå•æœç´¢"Ollama"å¹¶å¯åŠ¨

# æˆ–å‘½ä»¤è¡Œå¯åŠ¨
ollama serve
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥

**è§£å†³**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒï¼ˆå¦‚æœéœ€è¦ï¼‰
export OLLAMA_HOST=https://ollama.mirror.cn

# æˆ–æ‰‹åŠ¨ä¸‹è½½åå¯¼å…¥
ollama create phi3:mini -f Modelfile
```

### é—®é¢˜3: æ¨ç†å¤ªæ…¢

**ç—‡çŠ¶**: æ¨ç† > 200ms

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨GPUåŠ é€Ÿ
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆgemma:2bï¼‰
3. å‡å°‘`num_predict`å‚æ•°
4. å‡çº§ç¡¬ä»¶

### é—®é¢˜4: JSONæ ¼å¼é”™è¯¯

**ç—‡çŠ¶**: 
```
âŒ JSONè§£æé”™è¯¯
```

**ä¸´æ—¶è§£å†³**:
AIæ¨¡å‹ä¼šè¿”å›é»˜è®¤å®‰å…¨åŠ¨ä½œï¼ˆè§’åº¦0ï¼‰

**é•¿æœŸè§£å†³**:
1. ä½¿ç”¨Grammarçº¦æŸï¼ˆllama.cppï¼‰
2. ä¼˜åŒ–Prompt
3. å¢åŠ åå¤„ç†ä¿®å¤

---

## âš™ï¸ é…ç½®ä¼˜åŒ–

### é™ä½å»¶è¿Ÿ

åœ¨ `ai_model.py` ä¸­è°ƒæ•´:
```python
options={
    'temperature': 0.05,  # é™ä½æ¸©åº¦
    'num_predict': 30,    # å‡å°‘ç”Ÿæˆtokenæ•°
    'top_p': 0.85
}
```

### æé«˜ç¨³å®šæ€§

```python
options={
    'temperature': 0.01,  # æä½æ¸©åº¦
    'top_k': 10,          # é™åˆ¶é‡‡æ ·èŒƒå›´
    'repeat_penalty': 1.1
}
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

| ç¡¬ä»¶é…ç½® | æ¨¡å‹ | æ¨ç†é€Ÿåº¦ | æ§åˆ¶é¢‘ç‡ |
|---------|------|---------|---------|
| i7-12700 | Phi-3 Q4 | 40-60ms | 25-30Hz âœ… |
| i5-10400 | Phi-3 Q4 | 80-120ms | 12-18Hz âš ï¸ |
| RTX 4060 | Phi-3 Q4 | 15-25ms | 40-50Hz ğŸ”¥ |

---

## ğŸ“ ä¸‹ä¸€æ­¥

å®Œæˆå®‰è£…å:

1. **è°ƒä¼˜Prompt** - æ”¹è¿› `_build_prompt()` æ–¹æ³•
2. **æ”¶é›†æ•°æ®** - è®°å½•æˆåŠŸçš„æ§åˆ¶è½¨è¿¹
3. **é›†æˆPID** - ç»“åˆPIDæ§åˆ¶å™¨
4. **æ·»åŠ 70Bä¼˜åŒ–å™¨** - å®ç°ç­–ç•¥ä¼˜åŒ–

---

> ğŸ’¡ **æç¤º**: ç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´è®©æ¨¡å‹"çƒ­èº«"ï¼Œåç»­æ¨ç†ä¼šæ›´å¿«ã€‚
