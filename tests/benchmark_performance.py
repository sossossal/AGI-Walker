"""
æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
æµ‹è¯•é€šä¿¡ã€åºåˆ—åŒ–ã€è®­ç»ƒæ€§èƒ½
"""

import time
import numpy as np
from typing import Dict, Any
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.results: Dict[str, Any] = {}
    
    def benchmark_serialization(self):
        """åºåˆ—åŒ–æ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“Š åºåˆ—åŒ–æ€§èƒ½æµ‹è¯•")
        print("="*60)
        
        import json
        try:
            import msgpack
            has_msgpack = True
        except ImportError:
            has_msgpack = False
        
        # æµ‹è¯•æ•°æ®
        test_data = {
            "joint_pos": np.random.randn(8).tolist(),
            "joint_vel": np.random.randn(8).tolist(),
            "timestamp": time.time()
        }
        
        # JSON æµ‹è¯•
        json_times = []
        for _ in range(10000):
            start = time.perf_counter()
            serialized = json.dumps(test_data).encode()
            deserialized = json.loads(serialized.decode())
            json_times.append(time.perf_counter() - start)
        
        json_avg = np.mean(json_times) * 1e6
        print(f"JSON:    {json_avg:.2f} Î¼s (avg)")
        
        # msgpack æµ‹è¯•
        if has_msgpack:
            msgpack_times = []
            for _ in range(10000):
                start = time.perf_counter()
                serialized = msgpack.packb(test_data)
                deserialized = msgpack.unpackb(serialized)
                msgpack_times.append(time.perf_counter() - start)
            
            msgpack_avg = np.mean(msgpack_times) * 1e6
            speedup = json_avg / msgpack_avg
            
            print(f"msgpack: {msgpack_avg:.2f} Î¼s (avg)")
            print(f"æ€§èƒ½æå‡: {speedup:.2f}x")
            
            self.results["serialization"] = {
                "json_us": json_avg,
                "msgpack_us": msgpack_avg,
                "speedup": speedup
            }
        else:
            self.results["serialization"] = {"json_us": json_avg}
    
    def benchmark_zenoh_latency(self):
        """Zenoh é€šä¿¡å»¶è¿Ÿæµ‹è¯•"""
        print("\nğŸ“¡ Zenoh é€šä¿¡å»¶è¿Ÿæµ‹è¯•")
        print("="*60)
        
        try:
            from python_api.optimized_zenoh import OptimizedZenohInterface
            
            zenoh = OptimizedZenohInterface()
            
            # æµ‹è¯•å‘å¸ƒå»¶è¿Ÿ
            latencies = []
            zenoh.declare_publisher("bench/test")
            
            for i in range(1000):
                start = time.perf_counter()
                zenoh.publish("bench/test", {"data": i})
                latencies.append((time.perf_counter() - start) * 1e6)
            
            avg_latency = np.mean(latencies)
            p99_latency = np.percentile(latencies, 99)
            
            print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} Î¼s")
            print(f"P99 å»¶è¿Ÿ: {p99_latency:.2f} Î¼s")
            
            self.results["zenoh_latency"] = {
                "avg_us": avg_latency,
                "p99_us": p99_latency
            }
            
            zenoh.close()
            
        except Exception as e:
            print(f"âš ï¸ Zenoh æµ‹è¯•è·³è¿‡: {e}")
    
    def benchmark_parallel_training(self):
        """å¹¶è¡Œè®­ç»ƒæ€§èƒ½æµ‹è¯•"""
        print("\nğŸš€ å¹¶è¡Œè®­ç»ƒæ€§èƒ½æµ‹è¯•")
        print("="*60)
        
        try:
            from python_controller.parallel_trainer import ParallelTrainingManager
            import gymnasium as gym
            
            # å•è¿›ç¨‹åŸºå‡†
            print("å•è¿›ç¨‹è®­ç»ƒ...")
            start = time.time()
            env = gym.make('CartPole-v1')
            for _ in range(100):
                obs, _ = env.reset()
                for _ in range(200):
                    action = env.action_space.sample()
                    obs, _, terminated, truncated, _ = env.step(action)
                    if terminated or truncated:
                        break
            single_time = time.time() - start
            
            print(f"å•è¿›ç¨‹æ—¶é—´: {single_time:.2f}s")
            
            # å¤šè¿›ç¨‹ (ç®€åŒ–æµ‹è¯•)
            print("\nå¤šè¿›ç¨‹è®­ç»ƒ...")
            # æ³¨: å®Œæ•´æµ‹è¯•éœ€è¦æ›´å¤šæ—¶é—´
            
            self.results["parallel_training"] = {
                "single_process_time": single_time
            }
            
        except Exception as e:
            print(f"âš ï¸ å¹¶è¡Œè®­ç»ƒæµ‹è¯•è·³è¿‡: {e}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("æ€§èƒ½åŸºå‡†æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        if "serialization" in self.results:
            print("\nåºåˆ—åŒ–æ€§èƒ½:")
            ser = self.results["serialization"]
            print(f"  JSON:    {ser['json_us']:.2f} Î¼s")
            if "msgpack_us" in ser:
                print(f"  msgpack: {ser['msgpack_us']:.2f} Î¼s")
                print(f"  æå‡:    {ser['speedup']:.2f}x")
        
        if "zenoh_latency" in self.results:
            print("\nZenoh é€šä¿¡:")
            lat = self.results["zenoh_latency"]
            print(f"  å¹³å‡å»¶è¿Ÿ: {lat['avg_us']:.2f} Î¼s")
            print(f"  P99 å»¶è¿Ÿ: {lat['p99_us']:.2f} Î¼s")
            
            # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
            target = 2000  # 2ms = 2000Î¼s
            if lat['avg_us'] < target:
                print(f"  âœ… è¾¾æ ‡ (ç›®æ ‡: <{target} Î¼s)")
            else:
                print(f"  âŒ æœªè¾¾æ ‡ (ç›®æ ‡: <{target} Î¼s)")
        
        if "parallel_training" in self.results:
            print("\nå¹¶è¡Œè®­ç»ƒ:")
            par = self.results["parallel_training"]
            print(f"  å•è¿›ç¨‹: {par['single_process_time']:.2f}s")


def run_all_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    bench = PerformanceBenchmark()
    
    print("\nğŸ§ª AGI-Walker æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
    print("="*60)
    
    bench.benchmark_serialization()
    bench.benchmark_zenoh_latency()
    bench.benchmark_parallel_training()
    
    bench.print_summary()
    
    return bench.results


if __name__ == "__main__":
    results = run_all_benchmarks()
