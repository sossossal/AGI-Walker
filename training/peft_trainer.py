"""
PEFTå¾®è°ƒè®­ç»ƒå™¨
å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œæ”¯æŒå¤šç§æ–¹æ³•ï¼ˆPrefix Tuning, Adapterç­‰ï¼‰
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
from enum import Enum
import numpy as np


class PEFTMethod(Enum):
    """PEFTæ–¹æ³•"""
    LORA = "lora"
    PREFIX_TUNING = "prefix_tuning"
    ADAPTER = "adapter"
    PROMPT_TUNING = "prompt_tuning"


@dataclass 
class PEFTConfig:
    """PEFTé…ç½®"""
    method: PEFTMethod = PEFTMethod.PREFIX_TUNING
    
    # é€šç”¨å‚æ•°
    learning_rate: float = 1e-4
    batch_size: int = 8
    num_epochs: int = 3
    warmup_steps: int = 100
    
    # LoRAå‚æ•°
    lora_r: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    
    # Prefix Tuningå‚æ•°
    prefix_length: int = 20
    prefix_projection: bool = True
    
    # Adapterå‚æ•°
    adapter_size: int = 64
    adapter_dropout: float = 0.1


class PEFTTrainer:
    """
    å‚æ•°é«˜æ•ˆå¾®è°ƒè®­ç»ƒå™¨
    
    åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤šç§PEFTæ–¹æ³•
    2. ä½å†…å­˜æ¶ˆè€—
    3. å¿«é€Ÿå¾®è°ƒ
    """
    
    def __init__(
        self,
        base_model: str = "microsoft/phi-2",
        config: Optional[PEFTConfig] = None,
        output_dir: str = "d:/æ–°å»ºæ–‡ä»¶å¤¹/AGI-Walker/models/peft"
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            base_model: åŸºç¡€æ¨¡å‹åç§°/è·¯å¾„
            config: PEFTé…ç½®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.base_model = base_model
        self.config = config or PEFTConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹å’Œtokenizerï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._model = None
        self._tokenizer = None
        self._peft_model = None
        
        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.training_history = []
        
        print(f"âœ… PEFTè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   åŸºç¡€æ¨¡å‹: {base_model}")
        print(f"   æ–¹æ³•: {self.config.method.value}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _load_base_model(self):
        """åŠ è½½åŸºç¡€æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model}")
            
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.base_model,
                trust_remote_code=True
            )
            
            self._model = AutoModelForCausalLM.from_pretrained(
                self.base_model,
                trust_remote_code=True,
                torch_dtype="auto",
                device_map="auto"
            )
            
            # è®¾ç½®pad token
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _create_peft_config(self):
        """åˆ›å»ºPEFTé…ç½®"""
        try:
            from peft import (
                LoraConfig,
                PrefixTuningConfig, 
                AdapterConfig,
                PromptTuningConfig,
                get_peft_model,
                TaskType
            )
        except ImportError:
            print("âš ï¸ PEFTåº“æœªå®‰è£…")
            print("è¯·è¿è¡Œ: pip install peft")
            return None
        
        method = self.config.method
        
        if method == PEFTMethod.LORA:
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=["q_proj", "v_proj"]
            )
        
        elif method == PEFTMethod.PREFIX_TUNING:
            peft_config = PrefixTuningConfig(
                task_type=TaskType.CAUSAL_LM,
                num_virtual_tokens=self.config.prefix_length,
                prefix_projection=self.config.prefix_projection
            )
        
        elif method == PEFTMethod.ADAPTER:
            # æ³¨æ„ï¼šæŸäº›ç‰ˆæœ¬çš„PEFTå¯èƒ½ä¸æ”¯æŒ
            try:
                peft_config = AdapterConfig(
                    task_type=TaskType.CAUSAL_LM,
                    adapter_size=self.config.adapter_size
                )
            except:
                print("âš ï¸ Adapteré…ç½®ä¸å¯ç”¨ï¼Œå›é€€åˆ°LoRA")
                peft_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM,
                    r=self.config.lora_r,
                    lora_alpha=self.config.lora_alpha
                )
        
        elif method == PEFTMethod.PROMPT_TUNING:
            peft_config = PromptTuningConfig(
                task_type=TaskType.CAUSAL_LM,
                num_virtual_tokens=self.config.prefix_length
            )
        
        else:
            raise ValueError(f"æœªçŸ¥çš„PEFTæ–¹æ³•: {method}")
        
        return peft_config
    
    def prepare_dataset(
        self,
        labeled_data_path: str,
        max_length: int = 512
    ):
        """
        å‡†å¤‡å¾®è°ƒæ•°æ®é›†
        
        Args:
            labeled_data_path: æ ‡è®°æ•°æ®è·¯å¾„
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        
        Returns:
            Datasetå¯¹è±¡
        """
        try:
            from datasets import Dataset
        except ImportError:
            print("âš ï¸ datasetsåº“æœªå®‰è£…")
            print("è¯·è¿è¡Œ: pip install datasets")
            return None
        
        print(f"å‡†å¤‡æ•°æ®é›†: {labeled_data_path}")
        
        # åŠ è½½æ ‡è®°æ•°æ®
        with open(labeled_data_path, 'r', encoding='utf-8') as f:
            labeled_data = json.load(f)
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        train_examples = []
        
        for item in labeled_data:
            # æ„å»ºè®­ç»ƒæ–‡æœ¬
            text = self._format_training_example(item)
            train_examples.append({"text": text})
        
        print(f"   æ ·æœ¬æ•°: {len(train_examples)}")
        
        # åˆ›å»ºDataset
        dataset = Dataset.from_list(train_examples)
        
        # Tokenize
        if self._tokenizer is None:
            self._load_base_model()
        
        def tokenize_function(examples):
            return self._tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_length,
                padding="max_length"
            )
        
        dataset = dataset.map(tokenize_function, batched=True)
        
        return dataset
    
    def _format_training_example(self, item: dict) -> str:
        """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬"""
        label = item.get('label', 'unknown')
        explanation = item.get('explanation', '')
        metadata = item.get('metadata', {})
        
        # æ„å»ºè¾“å…¥-è¾“å‡ºå¯¹
        input_text = f"""åˆ†ææœºå™¨äººè½¨è¿¹ï¼š
- æŒç»­æ—¶é—´: {metadata.get('duration', 0)}æ­¥
- æœ€å¤§Roll: {metadata.get('max_roll', 0):.1f}Â°
- æœ€å¤§Pitch: {metadata.get('max_pitch', 0):.1f}Â°
- æœ€ä½é«˜åº¦: {metadata.get('min_height', 1.5):.2f}m

æ ‡ç­¾: {label}
è§£é‡Š: {explanation}"""
        
        return input_text
    
    def train(
        self,
        dataset,
        eval_dataset=None,
        resume_from_checkpoint: bool = False
    ) -> dict:
        """
        æ‰§è¡ŒPEFTå¾®è°ƒ
        
        Args:
            dataset: è®­ç»ƒæ•°æ®é›†
            eval_dataset: éªŒè¯æ•°æ®é›†
            resume_from_checkpoint: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        
        Returns:
            è®­ç»ƒç»“æœ
        """
        if self._model is None:
            self._load_base_model()
        
        try:
            from peft import get_peft_model
            from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling
        except ImportError:
            print("âŒ ç¼ºå°‘å¿…è¦çš„åº“")
            return {"error": "missing dependencies"}
        
        print("\nğŸš€ å¼€å§‹PEFTå¾®è°ƒ")
        print(f"   æ–¹æ³•: {self.config.method.value}")
        print(f"   Epochs: {self.config.num_epochs}")
        print(f"   æ‰¹é‡å¤§å°: {self.config.batch_size}")
        
        start_time = time.time()
        
        # åˆ›å»ºPEFTæ¨¡å‹
        peft_config = self._create_peft_config()
        if peft_config is None:
            return {"error": "PEFT config creation failed"}
        
        self._peft_model = get_peft_model(self._model, peft_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in self._peft_model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in self._peft_model.parameters())
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.output_dir / "checkpoints"),
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=10,
            save_steps=100,
            save_total_limit=2,
            fp16=True,
            report_to="none"
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self._tokenizer,
            mlm=False
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self._peft_model,
            args=training_args,
            train_dataset=dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator
        )
        
        # è®­ç»ƒ
        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)
        
        training_time = time.time() - start_time
        
        # ä¿å­˜
        self._peft_model.save_pretrained(self.output_dir / "final")
        self._tokenizer.save_pretrained(self.output_dir / "final")
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {self.output_dir / 'final'}")
        
        self.is_trained = True
        
        result = {
            "training_time": training_time,
            "trainable_params": trainable_params,
            "trainable_ratio": trainable_params / all_params,
            "method": self.config.method.value,
            "model_path": str(self.output_dir / "final")
        }
        
        self.training_history.append(result)
        
        return result
    
    def merge_and_export(
        self,
        output_path: Optional[str] = None
    ) -> str:
        """
        åˆå¹¶é€‚é…å™¨å¹¶å¯¼å‡ºå®Œæ•´æ¨¡å‹
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        
        Returns:
            å¯¼å‡ºè·¯å¾„
        """
        if self._peft_model is None:
            raise RuntimeError("æ¨¡å‹æœªè®­ç»ƒ")
        
        if output_path is None:
            output_path = self.output_dir / "merged"
        
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"åˆå¹¶æ¨¡å‹åˆ°: {output_path}")
        
        try:
            # åˆå¹¶æƒé‡
            merged_model = self._peft_model.merge_and_unload()
            
            # ä¿å­˜
            merged_model.save_pretrained(output_path)
            self._tokenizer.save_pretrained(output_path)
            
            print(f"âœ… åˆå¹¶å®Œæˆ: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
            return ""
    
    def load_peft_model(self, path: str):
        """åŠ è½½å·²è®­ç»ƒçš„PEFTæ¨¡å‹"""
        try:
            from peft import PeftModel
            
            if self._model is None:
                self._load_base_model()
            
            self._peft_model = PeftModel.from_pretrained(self._model, path)
            self.is_trained = True
            
            print(f"âœ… PEFTæ¨¡å‹å·²åŠ è½½: {path}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 100
    ) -> str:
        """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆ"""
        if self._peft_model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
        
        inputs = self._tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(self._peft_model.device) for k, v in inputs.items()}
        
        outputs = self._peft_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7
        )
        
        response = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "base_model": self.base_model,
            "method": self.config.method.value,
            "is_trained": self.is_trained,
            "training_history": self.training_history,
            "output_dir": str(self.output_dir)
        }


def create_peft_trainer(
    base_model: str = "microsoft/phi-2",
    method: str = "prefix_tuning"
) -> PEFTTrainer:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºPEFTè®­ç»ƒå™¨"""
    config = PEFTConfig(method=PEFTMethod(method))
    return PEFTTrainer(base_model, config)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("PEFTå¾®è°ƒè®­ç»ƒå™¨æµ‹è¯•\n")
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä½¿ç”¨è™šæ‹Ÿæ¨¡å¼æµ‹è¯•ï¼‰
    print("=== é…ç½®ä¿¡æ¯ ===")
    config = PEFTConfig(
        method=PEFTMethod.PREFIX_TUNING,
        num_epochs=1,
        batch_size=4
    )
    print(f"æ–¹æ³•: {config.method.value}")
    print(f"Prefixé•¿åº¦: {config.prefix_length}")
    print(f"å­¦ä¹ ç‡: {config.learning_rate}")
    
    # æ¨¡æ‹Ÿç»Ÿè®¡
    print("\n=== å‚æ•°æ•ˆç‡å¯¹æ¯” ===")
    print("| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | å†…å­˜èŠ‚çœ |")
    print("|------|-----------|---------|")
    print("| Full FT | 100% | 0% |")
    print("| LoRA | ~0.1% | ~90% |")
    print("| Prefix | ~0.1% | ~95% |")
    print("| Adapter | ~2% | ~80% |")
