"""
æ•°æ®é›†æ¸…æ´—å·¥å…·
å»é‡ã€è¿‡æ»¤ã€ç±»åˆ«å¹³è¡¡å’Œæ•°æ®å¢å¼º
"""

import json
import random
from pathlib import Path
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass
from collections import Counter
import numpy as np
import hashlib


@dataclass
class CleaningStats:
    """æ¸…æ´—ç»Ÿè®¡"""
    original_count: int = 0
    after_dedup: int = 0
    after_filter: int = 0
    after_balance: int = 0
    after_augment: int = 0


class DatasetCleaner:
    """
    æ•°æ®é›†æ¸…æ´—å·¥å…·
    
    åŠŸèƒ½ï¼š
    1. å»é‡
    2. è´¨é‡è¿‡æ»¤
    3. ç±»åˆ«å¹³è¡¡
    4. æ•°æ®å¢å¼º
    """
    
    def __init__(self, seed: int = 42):
        random.seed(seed)
        np.random.seed(seed)
        self.stats = CleaningStats()
    
    def clean_pipeline(
        self,
        dataset: List[dict],
        quality_threshold: float = 0.3,
        balance: bool = True,
        augment_factor: int = 1
    ) -> List[dict]:
        """
        å®Œæ•´æ¸…æ´—ç®¡é“
        
        Args:
            dataset: åŸå§‹æ•°æ®é›†
            quality_threshold: è´¨é‡é˜ˆå€¼
            balance: æ˜¯å¦å¹³è¡¡ç±»åˆ«
            augment_factor: å¢å¼ºå€æ•°
        
        Returns:
            æ¸…æ´—åçš„æ•°æ®é›†
        """
        print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        print(f"   åŸå§‹æ ·æœ¬æ•°: {len(dataset)}")
        
        self.stats.original_count = len(dataset)
        
        # 1. å»é‡
        dataset = self.remove_duplicates(dataset)
        self.stats.after_dedup = len(dataset)
        print(f"   å»é‡å: {len(dataset)}")
        
        # 2. è´¨é‡è¿‡æ»¤
        dataset = self.filter_by_quality(dataset, quality_threshold)
        self.stats.after_filter = len(dataset)
        print(f"   è¿‡æ»¤å: {len(dataset)}")
        
        # 3. ç±»åˆ«å¹³è¡¡
        if balance:
            dataset = self.balance_classes(dataset)
            self.stats.after_balance = len(dataset)
            print(f"   å¹³è¡¡å: {len(dataset)}")
        
        # 4. æ•°æ®å¢å¼º
        if augment_factor > 1:
            dataset = self.augment(dataset, augment_factor)
            self.stats.after_augment = len(dataset)
            print(f"   å¢å¼ºå: {len(dataset)}")
        
        print(f"âœ… æ¸…æ´—å®Œæˆ")
        return dataset
    
    def remove_duplicates(self, dataset: List[dict]) -> List[dict]:
        """å»é‡"""
        seen_hashes = set()
        unique_samples = []
        
        for sample in dataset:
            # è®¡ç®—æ ·æœ¬å“ˆå¸Œ
            sample_hash = self._compute_hash(sample)
            
            if sample_hash not in seen_hashes:
                seen_hashes.add(sample_hash)
                unique_samples.append(sample)
        
        return unique_samples
    
    def _compute_hash(self, sample: dict) -> str:
        """è®¡ç®—æ ·æœ¬å“ˆå¸Œ"""
        # ä½¿ç”¨å…³é”®å­—æ®µè®¡ç®—å“ˆå¸Œ
        key_fields = ['label', 'metadata']
        hash_input = json.dumps(
            {k: sample.get(k) for k in key_fields},
            sort_keys=True
        )
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def filter_by_quality(
        self,
        dataset: List[dict],
        threshold: float = 0.3
    ) -> List[dict]:
        """æŒ‰è´¨é‡è¿‡æ»¤"""
        filtered = []
        
        for sample in dataset:
            quality = sample.get('quality_score', 0.5)
            
            if quality >= threshold:
                filtered.append(sample)
        
        return filtered
    
    def filter_by_condition(
        self,
        dataset: List[dict],
        condition: Callable[[dict], bool]
    ) -> List[dict]:
        """æŒ‰è‡ªå®šä¹‰æ¡ä»¶è¿‡æ»¤"""
        return [s for s in dataset if condition(s)]
    
    def balance_classes(
        self,
        dataset: List[dict],
        strategy: str = "oversample"
    ) -> List[dict]:
        """
        ç±»åˆ«å¹³è¡¡
        
        Args:
            dataset: æ•°æ®é›†
            strategy: ç­–ç•¥ ("oversample", "undersample", "both")
        
        Returns:
            å¹³è¡¡åçš„æ•°æ®é›†
        """
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        label_counts = Counter(s.get('label', 'unknown') for s in dataset)
        
        if not label_counts:
            return dataset
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        by_label = {}
        for sample in dataset:
            label = sample.get('label', 'unknown')
            if label not in by_label:
                by_label[label] = []
            by_label[label].append(sample)
        
        if strategy == "undersample":
            # ä¸‹é‡‡æ ·åˆ°æœ€å°ç±»åˆ«æ•°é‡
            min_count = min(label_counts.values())
            balanced = []
            for label, samples in by_label.items():
                balanced.extend(random.sample(samples, min(len(samples), min_count)))
        
        elif strategy == "oversample":
            # ä¸Šé‡‡æ ·åˆ°æœ€å¤§ç±»åˆ«æ•°é‡
            max_count = max(label_counts.values())
            balanced = []
            for label, samples in by_label.items():
                if len(samples) < max_count:
                    # é‡å¤é‡‡æ ·
                    oversampled = samples.copy()
                    while len(oversampled) < max_count:
                        oversampled.append(random.choice(samples))
                    balanced.extend(oversampled)
                else:
                    balanced.extend(samples)
        
        else:  # both
            # é‡‡æ ·åˆ°ä¸­ä½æ•°
            median_count = int(np.median(list(label_counts.values())))
            balanced = []
            for label, samples in by_label.items():
                if len(samples) > median_count:
                    balanced.extend(random.sample(samples, median_count))
                elif len(samples) < median_count:
                    oversampled = samples.copy()
                    while len(oversampled) < median_count:
                        oversampled.append(random.choice(samples))
                    balanced.extend(oversampled)
                else:
                    balanced.extend(samples)
        
        random.shuffle(balanced)
        return balanced
    
    def augment(
        self,
        dataset: List[dict],
        factor: int = 2
    ) -> List[dict]:
        """
        æ•°æ®å¢å¼º
        
        Args:
            dataset: æ•°æ®é›†
            factor: å¢å¼ºå€æ•°
        
        Returns:
            å¢å¼ºåçš„æ•°æ®é›†
        """
        augmented = dataset.copy()
        
        for _ in range(factor - 1):
            for sample in dataset:
                aug_sample = self._augment_sample(sample)
                augmented.append(aug_sample)
        
        return augmented
    
    def _augment_sample(self, sample: dict) -> dict:
        """å¢å¼ºå•ä¸ªæ ·æœ¬"""
        import copy
        aug = copy.deepcopy(sample)
        
        # ä¿®æ”¹ID
        aug['id'] = f"{sample.get('id', 'sample')}_aug_{random.randint(1000, 9999)}"
        
        # æ·»åŠ å™ªå£°åˆ°å…ƒæ•°æ®
        metadata = aug.get('metadata', {})
        
        if 'max_roll' in metadata:
            metadata['max_roll'] += random.gauss(0, 1)
        if 'max_pitch' in metadata:
            metadata['max_pitch'] += random.gauss(0, 1)
        if 'duration' in metadata:
            metadata['duration'] = max(1, metadata['duration'] + random.randint(-5, 5))
        
        aug['metadata'] = metadata
        aug['_augmented'] = True
        
        return aug
    
    def split_dataset(
        self,
        dataset: List[dict],
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1
    ) -> tuple:
        """åˆ’åˆ†æ•°æ®é›†"""
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.01
        
        # æ‰“ä¹±
        shuffled = dataset.copy()
        random.shuffle(shuffled)
        
        n = len(shuffled)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        train = shuffled[:train_end]
        val = shuffled[train_end:val_end]
        test = shuffled[val_end:]
        
        return train, val, test
    
    def get_label_distribution(self, dataset: List[dict]) -> Dict[str, int]:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        return dict(Counter(s.get('label', 'unknown') for s in dataset))
    
    def save_dataset(
        self,
        dataset: List[dict],
        output_path: str
    ):
        """ä¿å­˜æ•°æ®é›†"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    def get_stats(self) -> dict:
        """è·å–æ¸…æ´—ç»Ÿè®¡"""
        return {
            "original_count": self.stats.original_count,
            "after_dedup": self.stats.after_dedup,
            "after_filter": self.stats.after_filter,
            "after_balance": self.stats.after_balance,
            "after_augment": self.stats.after_augment,
            "dedup_removed": self.stats.original_count - self.stats.after_dedup,
            "filter_removed": self.stats.after_dedup - self.stats.after_filter
        }


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æ•°æ®é›†æ¸…æ´—å·¥å…·æµ‹è¯•\n")
    
    # åˆ›å»ºæ¸…æ´—å™¨
    cleaner = DatasetCleaner(seed=42)
    
    # æ¨¡æ‹Ÿæ•°æ®é›†
    dataset = []
    labels = ["successful_gait", "fall_forward", "unstable_balance", "energy_inefficient"]
    
    # ä¸å¹³è¡¡åˆ†å¸ƒ
    for i in range(100):
        label = random.choices(labels, weights=[60, 15, 15, 10])[0]
        dataset.append({
            "id": f"sample_{i}",
            "label": label,
            "quality_score": random.uniform(0.2, 1.0),
            "metadata": {
                "max_roll": random.uniform(0, 30),
                "max_pitch": random.uniform(0, 30),
                "duration": random.randint(10, 200)
            }
        })
    
    # æ·»åŠ é‡å¤
    dataset.append(dataset[0].copy())
    dataset.append(dataset[1].copy())
    
    print("=== åŸå§‹åˆ†å¸ƒ ===")
    print(cleaner.get_label_distribution(dataset))
    
    # æ¸…æ´—
    cleaned = cleaner.clean_pipeline(
        dataset,
        quality_threshold=0.3,
        balance=True,
        augment_factor=1
    )
    
    print("\n=== æ¸…æ´—ååˆ†å¸ƒ ===")
    print(cleaner.get_label_distribution(cleaned))
    
    # åˆ’åˆ†
    train, val, test = cleaner.split_dataset(cleaned)
    print(f"\n=== æ•°æ®åˆ’åˆ† ===")
    print(f"è®­ç»ƒé›†: {len(train)}")
    print(f"éªŒè¯é›†: {len(val)}")
    print(f"æµ‹è¯•é›†: {len(test)}")
    
    # ç»Ÿè®¡
    print("\n=== æ¸…æ´—ç»Ÿè®¡ ===")
    print(json.dumps(cleaner.get_stats(), indent=2))
